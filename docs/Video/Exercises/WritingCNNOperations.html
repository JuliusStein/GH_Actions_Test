

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Neural Network Operations: Convolution and Pooling &mdash; CogWorks</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/language_data.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/my_theme.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> CogWorks
          

          
          </a>

          
            
            
              <div class="version">
                1.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Table of Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../intro.html">CogWorks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../pre_reqs.html">Course Pre-Requisites</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../supplemental_math.html">Supplemental Math Materials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../audio.html">Audio Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../vision.html">Vision Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../language.html">Language Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cheat_sheet.html">Cheat Sheets</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">CogWorks</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
      <li>Neural Network Operations: Convolution and Pooling</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/Video/Exercises/WritingCNNOperations.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container,
div.nbinput.container div.prompt,
div.nbinput.container div.input_area,
div.nbinput.container div[class*=highlight],
div.nbinput.container div[class*=highlight] pre,
div.nboutput.container,
div.nboutput.container div.prompt,
div.nboutput.container div.output_area,
div.nboutput.container div[class*=highlight],
div.nboutput.container div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    min-width: 5ex;
    padding-top: 0.3rem;
    padding-right: 0.3rem;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    background: #f5f5f5;
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 0.3rem;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="Neural-Network-Operations:-Convolution-and-Pooling">
<h1>Neural Network Operations: Convolution and Pooling<a class="headerlink" href="#Neural-Network-Operations:-Convolution-and-Pooling" title="Permalink to this headline">¶</a></h1>
<p>In our discussions throughout the rest of the course, we will often refer to convolution and pooling operations and use these as the basic building blocks of a substantial portion of our work. In this notebook, you will write your own basic convolution operation and apply it to an image. You’ll then compare your implementation to the convolution implementation in MyGrad. Finally, you will implement your own max-pooling operation and compare that implementation against the MyGrad implementation.</p>
<p>These operations all act on <em>windows</em> of an image. <code class="docutils literal notranslate"><span class="pre">mygrad.sliding_window_view</span></code> is very useful for this!</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import numpy as np
import mygrad
import mygrad as mg
%matplotlib notebook
img = mpimg.imread(&#39;./pics/meerkat.png&#39;)

fig, ax = plt.subplots()
ax.imshow(img);
</pre></div>
</div>
</div>
<p>Let’s visualize “windowing” this image using shape-(24, 24) windows, strided along H and W with a step size of 24. (Try changing these parameters).</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>from mygrad import sliding_window_view
x  = img.transpose(2, 0, 1)  # (H, W, C) --&gt; (C, H, W)  (we window over the trailing dimensions)
windowed_img = sliding_window_view(x, window_shape=(24, 24), step=24)
windowed_img.shape
</pre></div>
</div>
</div>
<p>According to the shape of <code class="docutils literal notranslate"><span class="pre">windowed_img</span></code>, we placed each shape-(24, 24) window at 11x11 locations. Let’s visualize this, plotting each of the 11x11 window placements.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>fig,ax = plt.subplots(nrows=windowed_img.shape[0], ncols=windowed_img.shape[1])
for i,j in np.ndindex(windowed_img.shape[:2]):
    ax[i,j].imshow(windowed_img[i,j].transpose(1, 2, 0))  # (C, Hw, Ww) -&gt; (Hw, Ww, C)
    ax[i,j].axis(&#39;off&#39;)
</pre></div>
</div>
</div>
<p>Try using different window-shape and stride combinations. Like window-(48, 48) with stride-(12,)</p>
</div>
<div class="section" id="Writing-Your-Own-Convolution-Function">
<h1>Writing Your Own Convolution Function<a class="headerlink" href="#Writing-Your-Own-Convolution-Function" title="Permalink to this headline">¶</a></h1>
<p>The first thing we’ll need to do in order to perform convolution is figure out what our output shape is going to be, given our input shape and our filter shape. Recall that we’ll be sliding our convolutional filter across the image at every valid location with a given stride. Then our output shape will be</p>
<div class="math notranslate nohighlight">
\begin{equation}
shape_\text{out} = \frac{shape_\text{in} - shape_\text{filter}}{stride} + 1
\end{equation}</div><p>It is important to verify that we get valid dimensions here: we can’t have fractional or negative sizes. Let’s define a function now that will compute the output shape:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def get_outshape(x_shape, w_shape, stride):
    &#39;&#39;&#39; Compute the shape of the output tensor given an input shape, convolutional
    filter shape, and stride.

    Parameters
    ----------
    x_shape : Tuple[int, int]
        The shape of the input tensor.

    w_shape : Tuple[int, int]
        The shape of the convolutional filter.

    stride : Tuple[int, int]
        The stride at which to apply the convolutional filter to the input.

    Returns
    -------
    numpy.ndarray[int], shape=(2,)
        The shape of the output tensor resulting from convolving a tensor of shape `x_shape`
        with a tensor of shape `w_shape`.
    &#39;&#39;&#39;
    x_shape = np.array(x_shape)
    w_shape = np.array(w_shape)
    stride = np.array(stride)

    out_shape = (x_shape - w_shape) / stride + 1

    if not all(i.is_integer() and i &gt; 0 for i in out_shape):
        msg = &quot;Stride and kernel dimensions are incompatible: \n&quot;
        msg += &quot;Input dimensions: {}\n&quot;.format(tuple(x_shape))
        msg += &quot;Stride dimensions: {}\n&quot;.format(tuple(stride))
        msg += &quot;Kernel dimensions: {}\n&quot;.format(tuple(w_shape))
        raise ValueError(msg)
    return out_shape.astype(np.int32)
</pre></div>
</div>
</div>
<p>We should perform a sanity check to verify that our function is working correctly. We’ll try a few test cases.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>assert tuple(get_outshape((10, 10), (3, 3), (1, 1))) == (8, 8)
assert tuple(get_outshape((11, 11), (3, 3), (2, 2))) == (5, 5)
assert tuple(get_outshape((10, 10), (5, 5), (1, 1))) == (6, 6)
</pre></div>
</div>
</div>
<p>Now we can define a function that will perform our convolution. We’ll expect an image of shape <span class="math notranslate nohighlight">\((C, H, W)\)</span> (<span class="math notranslate nohighlight">\(C\)</span> color-channels, <span class="math notranslate nohighlight">\(H\)</span> pixels along the vertical, <span class="math notranslate nohighlight">\(W\)</span> pixels along the horizontal) and a convolutional filter of shape <span class="math notranslate nohighlight">\((C, H_f, W_f)\)</span>, along with a spatial stride of <span class="math notranslate nohighlight">\((s_y, s_x)\)</span>. We’ll compute our output shape, then construct an output array of the correct shape using our function above. Once we have our output array, we’ll step through it and perform
our convolution.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def convolve(input_image, conv_filter, stride=(1, 1)):
    &#39;&#39;&#39; Convolve `input_image` with `conv_filter` at a stride of `stride`.

    Parameters
    ----------
    input_image : numpy.ndarray, shape=(C, H, W)
        The input over which to perform convolution.

    conv_filter : numpy.ndarray, shape=(C, Hf, Wf)
        The convolutional filter to slide across the image.

    stride : Tuple[int, int], optional (default=(1, 1))
        The stride at which to apply `conv_filter` across `input_image`.

    Returns
    -------
    numpy.ndarray, shape=(H&#39;, W&#39;)
        The result of convolving `input_image` with `conv_filter` at a stride of `stride`,
        where (H&#39;, W&#39;) is the result of `get_outshape`.
    &#39;&#39;&#39;
    # STUDENT CODE HERE
</pre></div>
</div>
</div>
<p>Make up a 1x9x9 array of integer-valued floats and a 1x3x3 array of integer-valued floats, as your “image” and your “conv-filter” respectively. Perform the convolution for strides 1, 2, and/or 3 by hand, and then run your <code class="docutils literal notranslate"><span class="pre">convolve</span></code> function and check your results.</p>
<p>We now have a convolutional operator defined! However, we need to be able to apply a <em>bank</em> of filters to a <em>stack</em> of images. We can use the convolution operation we just defined, looping over <span class="math notranslate nohighlight">\(K\)</span> filters of shape <span class="math notranslate nohighlight">\((C, Hf, Wf)\)</span> for each of <span class="math notranslate nohighlight">\(N\)</span> images of shape <span class="math notranslate nohighlight">\((C, H, W)\)</span> to perform a full forward pass of a single CNN layer. Let’s define that function now.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def conv_bank(input_images, conv_filters, stride=(1, 1)):
    &#39;&#39;&#39; Convolve a bank of filters over a stack of images.

    Parameters
    ----------
    input_images : numpy.ndarray, shape=(N, C, H, W)
        The images over which to convolve our filters.

    conv_filters : numpy.ndarray, shape=(K, C, Hf, Wf)
        The convolutional filters to apply to the images.

    stride : Tuple[int, int], optional (default=(1, 1))
        The stride at which to apply each filter to the images.

    Returns
    -------
    numpy.ndarray, shape=(N, K, H&#39;, W&#39;)
        The result of convolving `input_image` with `conv_filter` at a stride of `stride`,
        where (H&#39;, W&#39;) is the result of `get_outshape`.
    &#39;&#39;&#39;
    # STUDENT CODE HERE
</pre></div>
</div>
</div>
<p>Now we can verify the behavior of our function the same way we did before; let’s generate a stack of shape-(4, 1, 9, 9) images and a shape-(1, 1, 3, 3) filter bank. Call <code class="docutils literal notranslate"><span class="pre">convolve</span></code> manually for each of the 4 images with this single filter. Check that <code class="docutils literal notranslate"><span class="pre">conv_bank</span></code> indeed applies the same convolution to each of the four images.</p>
<p>Now with all that work out of the way, let’s get a time comparison between using our function and MyGrad’s convolution function!</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>timing_images = np.random.rand(10, 3, 64, 64)
timing_filters = np.random.rand(20, 3, 3, 3)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>%%timeit
conv_bank(timing_images, timing_filters)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>from mygrad.nnet.layers import conv_nd
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>%%timeit
conv_nd(timing_images, timing_filters, stride=(1, 1))
</pre></div>
</div>
</div>
<p>We can see the clear utility of the optimization that has gone into MyGrad! All the clever tricks we can use to speed up convolution pay dividends given how much computation is required for the convolution operation.</p>
<p>Now we’ll move on to implementing a pooling operation. Max-pooling is very useful as a downsampling step to reduce the size of an image, for example. We’ll implement this here. At each location in our image, we will compute the maximum value in a <span class="math notranslate nohighlight">\(Hp\times Wp\)</span> window and only record that value; our image will thus be downsampled by a factor of <span class="math notranslate nohighlight">\(Hp\)</span> in the first spatial dimension and <span class="math notranslate nohighlight">\(Wp\)</span> in the second. First up, we’ll define a function that takes as input a <span class="math notranslate nohighlight">\(H\times W\)</span>
channel and a pooling shape of <span class="math notranslate nohighlight">\(Hp\times Wp\)</span>, and outputs a max-pooled channel.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def max_pool(input_image, pool_shape=(2, 2)):
    &#39;&#39;&#39; Perform max-pooling over a single channel of an image.

    Parameters
    ----------
    input_image : numpy.ndarray, shape=(H, W)
        The channel over which to perform max-pooling.

    pool_shape : Tuple[int, int], optional (default=(2, 2))
        The shape of the max-pool. `pool_shape[0]` is Hp, and `pool_shape[1]` is Wp.

    Returns
    -------
    numpy.ndarray, shape=(H&#39;, W&#39;)
        The result of max-pooling `input_image` with a pooling window of shape `pool_shape`,
        where H&#39; is (H / Hp) and W&#39; is (W / Wp)
    &#39;&#39;&#39;
    # STUDENT CODE HERE
</pre></div>
</div>
</div>
<p>We can visually inspect our function to make sure it’s doing the right thing:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>a = np.random.rand(4, 4)
print(a, &#39;\n&#39;)
print(max_pool(a))
</pre></div>
</div>
</div>
<p>Now we can define a function that loops through each image in a stack of images, then loops through each channel of each image to pool them, just like in the convolution example. Let’s define that function now:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def max_pool_stack(input_images, pool_shape=(2, 2)):
    &#39;&#39;&#39; Perform max-pooling over a stack of images.

    Parameters
    ----------
    input_images : numpy.ndarray, shape=(N, C, H, W)
        The images over which to perform max-pooling.

    pool_shape : Tuple[int, int], optional (default=(2, 2))
        The shape of the max-pool. `pool_shape[0]` is Hp, and `pool_shape[1]` is Wp.

    Returns
    -------
    numpy.ndarray, shape=(N, C, H&#39;, W&#39;)
        The result of max-pooling `input_image` with a pooling window of shape `pool_shape`,
        where H&#39; is (H / Hp) and W&#39; is (W / Wp)
    &#39;&#39;&#39;
    # STUDENT CODE HERE
</pre></div>
</div>
</div>
<p>Again, we can visually inspect this function:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>a = np.random.rand(2, 2, 4, 4)
print(a, &#39;\n&#39;)
print(max_pool_stack(a, (2, 2)))
</pre></div>
</div>
</div>
<p>Now let’s compare our implementation against MyGrad!</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>a = np.random.rand(20, 10, 64, 64)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>%%timeit
max_pool_stack(a, (2, 2))
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>from mygrad.nnet.layers import max_pool
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>%%timeit
max_pool(a, (2, 2), (2, 2))
</pre></div>
</div>
</div>
</div>
<div class="section" id="Image-Processing-Via-Convolution">
<h1>Image Processing Via Convolution<a class="headerlink" href="#Image-Processing-Via-Convolution" title="Permalink to this headline">¶</a></h1>
<p>Given the right kernel, performing a convolution can be a powerful technique for processing and manipulating images. We will see that simple 3x3 kernels can be constructed that: - detect edges in an image - sharpen an image - blur an image</p>
<p>In practice, it is found that neural networks can “organically” learn some of these filter patterns - meaning that it learns to leverage some of these processed image features as it is learning!</p>
<p>The following are 3x3 kernels.</p>
<p>For edge detection:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">edge</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                 <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                 <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]])</span>
</pre></div>
</div>
<p>To sharpen images:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sharp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>  <span class="mi">0</span><span class="p">],</span>
                  <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                  <span class="p">[</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>  <span class="mi">0</span><span class="p">]])</span>
</pre></div>
</div>
<p>To blur images:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">gauss_blur</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                       <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
                       <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="n">gauss_blur</span> <span class="o">=</span> <span class="n">gauss_blur</span> <span class="o">/</span> <span class="mi">16</span>
</pre></div>
</div>
<p>Let’s use MyGrad’s <code class="docutils literal notranslate"><span class="pre">conv_nd</span></code> function (or our own implementation, as they both can perform convolutions) to test these filters.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># plot the image with matplotlib
img = mpimg.imread(&#39;./pics/meerkat.png&#39;)

fig, ax = plt.subplots()
ax.imshow(img);
</pre></div>
</div>
</div>
<p>What is the shape of this image-array? Which axis contains the color channels?</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def image_process_conv(img, kernel):
    &quot;&quot;&quot; This is a convenience function that allows us to use mygrad&#39;s nn-style
    convolution on a single 2D image with a single 2D kernel, without
    collapsing the color channels.

    matplotlib&#39;s imshow requires the image axes to be ordered as: (H, W, C),
    thus we must do some transposing.

    Parameters
    ----------
    img : numpy.ndarray, shape=(H, W, C)
    kernel : numpy.ndarray, shape=(Hf, Wf)

    Returns
    -------
    convolved_img : numpy.ndarray, shape=(H&#39;, W&#39;, C)&quot;&quot;&quot;
    # (H, W, C) --&gt; (C, 1, H, W)
    x = img.transpose(2,0,1)[:, np.newaxis, :, :]

    # (Hf, Wf) --&gt; (1, 1, Hf, Wf)
    kernel = kernel.reshape(1, 1, *kernel.shape)

    # conv: (C, 1, H, W) w/ (1, 1, Hf, Wf) --&gt; (C, 1, H&#39;, W&#39;)
    # squeeze + transpose: (C, 1, H&#39;, W&#39;) --&gt; (H&#39;, W&#39;, C)
    return conv_nd(x, kernel, stride=(1, 1)).data.squeeze().transpose(1, 2, 0)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># edge detection
edge_detect_kernel = np.array([[-1, -1, -1],
                               [-1,  8, -1],
                               [-1, -1, -1]])

fig, ax = plt.subplots()
ax.imshow(image_process_conv(img, edge_detect_kernel));
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>sharpening_kernel = np.array([[ 0, -1,  0],
                              [-1,  5, -1],
                              [ 0, -1,  0]])

fig, ax = plt.subplots()
ax.imshow(image_process_conv(img, sharpening_kernel));
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>gauss_blur = np.array([[1, 2, 1],
                       [2, 4, 2],
                       [1, 2, 1]])
gauss_blur = gauss_blur / 16
fig, ax = plt.subplots()
ax.imshow(image_process_conv(img, gauss_blur));
</pre></div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Ryan Soklaski

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>