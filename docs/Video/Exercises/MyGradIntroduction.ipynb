{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to MyGrad \n",
    "\n",
    "## Introducing MyGrad: Computing the Slope of a Simple Function\n",
    "\n",
    "[mygrad](https://mygrad.readthedocs.io/en/latest/index.html) is a so-called \"automatic differentiation\" (a.k.a \"autograd\") numerical library. This means that `mygrad` is able to calculate results from mathematical functions, and then evaluate the *derivatives* (slopes) of those functions. Note that `mygrad` cannot compute analytical derivatives (i.e. the function that describes the derivative at all points). `mygrad` can only find the derivative evaluated at specified values.\n",
    "\n",
    "Let's consider an exceedingly-simple function, $f(x) = 2x + 1$. This is a line with a slope of 2. Thus the derivative of this function should be $2$ for all values of $x$. I.e. $\\frac{df(x)}{dx} = 2$. Let's see that `mygrad` can produce this result.\n",
    "\n",
    "First we specify the point, $x$, at which we want to evaluate this function. Let's use $x = 5.0$, and compute $f(5.0)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "# run me\n",
    "import mygrad as mg\n",
    "\n",
    "# computing: f = 2x + 1, at x = 5.0\n",
    "x = mg.Tensor(5.0)\n",
    "f = 2*x + 1\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In computing $f(5.0)$, `mygrad` constructed a computational graph that tracks all of the numerical inputs and mathematical operations that were responsible for producing this result. Let's visualize the computational graph that is associated with $f(5.0)$:\n",
    "\n",
    "![title](pics/mygrad_graph.png)\n",
    "\n",
    "Because `mygrad` stores this computational graph, it knows how $f$ *depends* on $x$. This permits `mygrad` to answer the question:\n",
    "\n",
    ">\"if I increase $x$ slightly (infinitesimally) above $5.0$, by what proportion will $f$ change?\n",
    "\n",
    "or, equivalently:\n",
    "\n",
    ">\"holding all other variables fixed, what is the slope of $f$ at $x=5$?\n",
    "\n",
    "also equivalently, and more concisely:\n",
    "\n",
    ">\"What is $\\frac{df}{dx}\\Bigr\\rvert_{x = 5.0}$?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This computational graph is tied specifically to the the terminal node of the graph, `f`. Let's see that the \"creator\" of `f` is an instance of mygrad's `Add` class, as depicted above. Evaluate the attribute `f.creator` in the cell below (as it is just an attribute, and not a method, you don't need to \"call\" it - no parentheses needed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to compute the derivative of $f$ with respect to $x$; in `mygrad` we do this by invoking `f.backward()`. This says: \n",
    ">\"Starting with `f` traverse (backwards) through the graph and compute all the derivatives of $f$ with respect to any tensor in the graph.\"\n",
    "\n",
    "The values of these derivatives will be stored in `<var>.grad`, where `<var>` is any tensor in the graph. Thus `x.grad` will store the value $\\frac{df}{dx}\\Bigr\\rvert_{x = 5.0}$. Remind yourself, what should this value be?\n",
    "\n",
    "Invoke `f.backward()` and then inspect `x.grad` (which, like `<var>.creator`, is an attribute).\n",
    "Note that `x.grad` returns a numpy array, not a mygrad tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trigger back-propagation and check df/dx\n",
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leads us to a very important practical note: **once you have invoked back-propagation, MyGrad will automatically flush the computational graph involved, removing all** `creator` **references. If a tensor in the computational graph is then involved in another mathematical operation, its gradient will be set to None, to avoid unwittingly accumulating gradients**.\n",
    "\n",
    "Print `f.creator` now - is it the same as before calling `backward`?\n",
    "Check that the gradients of `x` and `f` are not `None`, then multiply `f` by `2`.\n",
    "What are the gradients of `f` and `x` now?\n",
    "Add `3` to `x`, and once again check the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What should $\\frac{df}{dx}\\Bigr\\rvert_{x = -10.0}$ be? Verify your intuition using `mygrad`, re-doing the steps from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing: f = 2x + 1, at x = -10.0 and checking df/dx\n",
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more interesting observation: after invoking back-propagation, inspect what `f.grad` is. This represents $\\frac{df}{df}$, or: \n",
    "\n",
    ">\"if I increase $f$ slightly (infinitesimally) above its present value, by what proportion will $f$ change?\n",
    "\n",
    "Given this description of $\\frac{df}{df}$, does the value for `f.grad` that you see make sense? Chat with a neighbor about this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary \n",
    "\n",
    "We have been introduced to `mygrad`, an auto-differentiation library. We saw that this library allows us to perform numerical calculations, and it stores a so-called computational graph that describes that that calculation. This permits `mygrad` to compute the derivatives of the terminal variable in that graph with respect to all of the other variables in that graph. It does this using the process of \"back-propagation\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding MyGrad's Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mygrad` has a [`Tensor`](https://mygrad.readthedocs.io/en/latest/tensor.html) object, which is nearly identical to NumPy's array; it:\n",
    "- stores data in an [N-dimensional array-like patterns](https://www.pythonlikeyoumeanit.com/Module3_IntroducingNumpy/AccessingDataAlongMultipleDimensions.html)\n",
    "- supports both [basic and advanced indexing](https://www.pythonlikeyoumeanit.com/Module3_IntroducingNumpy/BasicIndexing.html) \n",
    "- performs computations over arrays of numbers intuitively and efficiently, by leveraging [vectorization](https://www.pythonlikeyoumeanit.com/Module3_IntroducingNumpy/VectorizedOperations.html)\n",
    "- supports numpy's semantics of [broadcasting](https://www.pythonlikeyoumeanit.com/Module3_IntroducingNumpy/Broadcasting.html) operations between tensors of different shapes\n",
    "\n",
    "Then how do numpy and mygrad differ? Whereas numpy simply performs a computation as necessary and stores no information about which arrays participate in it, as we saw above, mygrad keeps track of the computational graph that its tensors participate in. This is what permits mygrad to perform auto-differentiation (via back-propagation), which is the central purpose of mygrad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, work through [this section of PLYMI](https://www.pythonlikeyoumeanit.com/Module3_IntroducingNumpy/IntroducingTheNDarray.html), but convert *all* of the numpy functions and objects to mygrad objects. E.g. instead of `import numpy as np`, write `import mygrad as mg`, and so on. \n",
    "\n",
    "You can use multiple cells for this to help organize your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> import numpy as np\n",
    "# >>> x = np.arange(9)\n",
    "# array([0, 1, 2, 3, 4, 5, 6, 7, 8])\n",
    "\n",
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # An ND-array belongs to the type `numpy.ndarray`\n",
    "# >>> type(x)\n",
    "# numpy.ndarray\n",
    "\n",
    "# >>> isinstance(x, np.ndarray)\n",
    "# True\n",
    "\n",
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> x = x.reshape(3,3)\n",
    "# >>> x\n",
    "# array([[0, 1, 2],\n",
    "#        [3, 4, 5],\n",
    "#        [6, 7, 8]])\n",
    "\n",
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> np.power(x, 2)  # can also be calculated using the shorthand: x**2\n",
    "# array([[ 0,  1,  4],\n",
    "#        [ 9, 16, 25],\n",
    "#        [36, 49, 64]], dtype=int32)\n",
    "\n",
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> np.mean(x, axis=1)\n",
    "# array([ 1.,  4.,  7.])\n",
    "\n",
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following cell, try using a numpy-array, `np.array([0., 1., 2.])`, as the exponential for the tensor `x`. It might be surprising that this works! What might the value be of using a numpy-array within a computational graph as opposed to a mygrad tensor? Talk to your neighbors about this and consult with an instructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> x ** np.array([0., 1., 2.])\n",
    "# array([[  1.,   1.,   4.],\n",
    "#        [  1.,   4.,  25.],\n",
    "#        [  1.,   7.,  64.]])\n",
    "\n",
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One distinct difference between numpy and mygrad is that, whereas you can access individual numbers from a numpy-array:\n",
    "\n",
    "```python\n",
    ">>> x = np.array([1, 2, 3])\n",
    ">>> x[1]  # returns an integer\n",
    "2\n",
    "```\n",
    "\n",
    "indexing into a `Tensor` will *always* return a `Tensor`:\n",
    "\n",
    "```python\n",
    ">>> x = mg.Tensor([1, 2, 3])\n",
    ">>> x[1]  # returns a 0-dimensional Tensor\n",
    "Tensor(2)\n",
    "```\n",
    "\n",
    "This is because mygrad has to be able to track all of the elements in all of the Tensors to reliable calculate derivatives for the computational graph. If you want to access the number, you can call `.item()` on a 0-dimensional Tensor (or a 0-dimensional numpy array):\n",
    "\n",
    "```python\n",
    ">>> x[1].item()\n",
    "2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read through the [documentation for mygrad's Tensor](https://mygrad.readthedocs.io/en/latest/tensor.html). Among the other details provided there, take note: what does `Tensor.data` store?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Many Derivatives at Once\n",
    "\n",
    "Compute $f(x) = 2x + 1$ on the domain $[-1, 1]$ sampling this domain evenly using $10$ points. Use a `mygrad` function instead of a numpy function (hint: many of the numpy creation functions such as `arange` and `linspace` are replicated in `mygrad`, but they return tensors instead of arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invoke back-propagation to compute the derivatives of $f$. What type of object is of `x.grad`? What is the shape of `x.grad`. Discuss with your neighbor the following questions: \n",
    "\n",
    "- what does each element of `x` represent?\n",
    "- what does each element of `f` represent?\n",
    "- what does each element of `x.grad` represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*SOLUTION HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot $f(x) = x^2$ on the domain $[-1, 1]$, sampling this domain evenly using $1,000$ points. In the same figure, plot $\\frac{df}{dx}$.\n",
    "It is suggested to **plot the underlying data** of each tensor; plotting a tensor itself after calling `backward` will null its gradients (i.e. set `<var>.grad = None`).\n",
    "\n",
    "Before you render your plot: what should the value of $\\frac{df}{dx}$ be at $x=0$ (what is the slope of $x^2$ at the origin?). \n",
    "\n",
    "What *sign* should $\\frac{df}{dx}$ have for $x < 0$? For $0 < x$? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reflecting on this plot, what is the slope of $f(x)$ at $x=-1$?. Does this plot reaffirm your interpretation of what `x.grad` represents for when `x` stores many numbers? Discuss with a neighbor. Flag an instructor if no one is quite sure.\n",
    "\n",
    "If you have not taken calculus before, can you deduce what the functional form is of $\\frac{df}{dx}$, for $f(x) = x^2$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*SOLUTION HERE*"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "notebook_metadata_filter": "nbsphinx",
   "text_representation": {
    "extension": ".md",
    "format_name": "markdown",
    "format_version": "1.3",
    "jupytext_version": "1.11.2"
   }
  },
  "kernelspec": {
   "display_name": "Python [conda env:week2]",
   "language": "python",
   "name": "conda-env-week2-py"
  },
  "nbsphinx": {
   "execute": "never"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
