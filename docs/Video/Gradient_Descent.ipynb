{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. meta::\n",
    "   :description: Topic: Computer Vision, Category: Introduction\n",
    "   :keywords: linear regression, supervised learning, gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Background Material**:\n",
    "\n",
    "It is highly recommended that the reader work through the introductions to [single variable calculus](https://rsokl.github.io/CogWeb/Math_Materials/Intro_Calc.html) and [multivariable calculus](https://rsokl.github.io/CogWeb/Math_Materials/Multivariable_Calculus.html) as a supplement to this section.\n",
    "These materials make accessible the most fundamental aspects of calculus needed to get a firm grasp on gradient-based learning.\n",
    "Even if you are already familiar with calculus, these sections also provide an introduction to automatic differentiation, which will be a critical technology for us moving forward.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient-Based Learning\n",
    "\n",
    "This section is an especially important one - it lays the foundation for understanding the \"learning\" in \"deep learning\".\n",
    "We will be introduced to an important but simple algorithm for numerical optimization known as **gradient descent**; this is the engine that drives the search for reliable model parameter values for many machine learning applications.\n",
    "\n",
    "Gradient descent is an iterative optimization algorithm by which we can **search for and locate the local minima of a function**.\n",
    "(Ideally, we would like to work with functions that are convex (bowl-shaped) and thus only have a single global minimum – and we want to find _that_ – but we will seldom find ourselves in such tidy circumstances).\n",
    "In the context of linear regression, which we were studying in the previous section, we would use gradient descent to search for the slope and $y$-intercept values that minimize the sum of squared residuals between out linear model and our recorded data.\n",
    "\n",
    "More generally – but still in the context of machine learning – we will use gradient descent to find the model parameter values that minimize a loss function (a.k.a objective function) that compares our model's predictions on observed data to known correct (or \"desired\") values.\n",
    "This minimization process, in the context of machine learning, very often is used to facilitate **supervised learning** for our model (more on this later).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Minimizing a Loss Function\n",
    "\n",
    "Although we are going to be studying the pure mathematical mechanics of gradient descent for both single variable and multivariable functions here, it is useful to keep our intentions clear and to use symbols that evoke the context of machine learning.\n",
    "$\\mathscr{L}$ **will represent our loss function**,\n",
    "meaning that it will provide a measure of the quality of our mathematical model's predictions by comparing them to desired or true outcomes.\n",
    "In particular we will always design $\\mathscr{L}$ such that **smaller values returned by** $\\mathscr{L}$ **always reflect better-quality predictions from our model**.\n",
    "\n",
    "$\\mathscr{L}$ will have two sources of inputs:\n",
    "\n",
    " 1. Our mathematical model's parameters (e.g. for our linear model, the slope $m$ and y-intercept $b$). **These are the variables that** $\\mathscr{L}$ **depends on**. Let's represent an arbitrary collection of $M$ model parameters as $(w_1, ..., w_M)$\n",
    " 2. The data and corresponding true data that we have collected and that we are fitting our model to. We do not want to manipulate this data in any way, so **the data is to be treated as a constant in the context of** $\\mathscr{L}$. Let's assume that we have $N$ pieces of observed data and corresponding \"truth\" values (e.g. if $w_{i}$ is a datum among our observations, then $y_{i}$ is the prediction that we _want_ our model to make about this datum). Thus we can represent this generic dataset as $(x_n, y_n)_{n=0}^{N-1}$.\n",
    "\n",
    "We will write our loss function as\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathscr{L}\\big(w_1, ..., w_M ; (x_n, y_n)_{n=0}^{N-1}\\big)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Thus, given our fixed dataset, we want to find the combination of model parameter values that minimize $\\mathscr{L}$; we will use gradient descent to achieve this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Reading Comprehension: Retracing Our Steps**:\n",
    "\n",
    "Given a simple linear model\n",
    "\n",
    "\\begin{equation}\n",
    "F\\big((w_1, w_2); x \\big) = w_2 x + w_1\n",
    "\\end{equation}\n",
    "\n",
    "Assuming that we have $N$ pieces of recorded observations and associated \"true\" outcomes, $(x_n, y_n)_{n=0}^{N-1}$, **write the \"mean-squared error\" loss function** in terms of the model's parameters, and the individual pieces of data.\n",
    "\n",
    "Refer back to the section on linear regression for a refresher on this particular loss function.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Overview of Gradient Descent"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<div style=\"text-align: center\">\n",
    "<p>\n",
    "<img src=\"../_images/gradient_descent_overview.png\" alt=\"A diagram describing gradient descent\" width=\"600\">\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that the function we seek to minimize is smooth and continuous such that we can evaluate its derivative anywhere in its input domain (consider, for instance, a parabola).\n",
    "Suppose, for simplicity's sake, that we are working with a single variable function, $\\mathscr{L}(w)$.\n",
    "Given some current value of $w$ ($w_\\mathrm{old}$), gradient descent provides us with a process for finding a nearby value of $w$ ($w_\\mathrm{new}$) such that $\\mathscr{L}(w_\\mathrm{new}) < \\mathscr{L}(w_\\mathrm{old})$.\n",
    "\n",
    "The following equation prescribes a single iteration for updating the value of $w$ that serves as our current \"best guess\" for the location of a minimum of $\\mathscr{L}$; it specifies a single \"step\" of gradient descent for a single variable function:\n",
    "\n",
    "\\begin{equation}\n",
    "w_\\mathrm{new} = w_\\mathrm{old} - \\delta \\frac{\\mathrm{d}\\mathscr{L}}{\\mathrm{d}w}\\bigg|_{w=w_\\mathrm{old}}.\n",
    "\\end{equation}\n",
    "\n",
    "Here $\\delta$ is a _positive number_ – a constant – that we specify; it is often called the the **learning rate**, as it scales the magnitude of the update made to $w_\\mathrm{old}$. Note that we often choose our very first value of $w_\\mathrm{old}$ by literally picking a random number. We often pick some reasonably-small number from some statistical distribution (tons of caveats to be made here, which we will get to later).\n",
    "\n",
    "The quantity $\\frac{\\mathrm{d}\\mathscr{L}}{\\mathrm{d}w}|_{w=w_\\mathrm{old}}$ is the (instantaneous) slope of $\\mathscr{L}(w)$ _at the point_ $w_\\mathrm{old}$; if it is a positive number, then, by definition, slightly increasing $w$ from $w_\\mathrm{old}$ will produce an increase in the value of $\\mathscr{L}(w)$.\n",
    "Thus $-\\frac{\\mathrm{d}\\mathscr{L}}{\\mathrm{d}w}|_{w=w_\\mathrm{old}}$  (note the negative sign) says: \"move $w$ in the direction that _decreases_ $\\mathscr{L}(w)$ in the neighborhood of that point.\"\n",
    "Hence this process will guide our updates to $w$ so as to progressively _descend_ $\\mathscr{L}(w)$.\n",
    "\n",
    "We will see that derivative of the single-variable function $\\mathscr{L}(w)$ plays the same role as the gradient of multivariable functions; this is why the process is generally called \"gradient descent\" and not \"derivative descent\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descending a Parabola\n",
    "\n",
    "Let's make gradient descent a bit more concrete.\n",
    "Suppose we want to find the global minimum of $\\mathscr{L}(w) = w^2$ (we known that this function is convex, thus it has a single global minimum and no other flat regions).\n",
    "We already know that the minimum for this function lies at $w = 0$ (it is immediately apparent simply from plotting the function), but it is useful regardless to see that gradient descent can find this minimum.\n",
    "\n",
    "We must keep in mind that gradient descent requires that we be able to evaluate $\\frac{\\mathrm{d}\\mathscr{L}}{\\mathrm{d}w}$ at any value of $w$.\n",
    "For $\\mathscr{L}(w)=w^2$, [its derivative is simply](https://rsokl.github.io/CogWeb/Math_Materials/Intro_Calc.html#Common-Derivatives) $\\frac{\\mathrm{d}\\mathscr{L}}{\\mathrm{d}w} = 2w$, and so evaluating the derivative is trivial!\n",
    "For more complicated functions, we will be grateful to have autodifferentiation libraries, which will evaluate derivatives for us without our having to do any calculus by hand.\n",
    "\n",
    "Let's start off with $w_\\mathrm{start}=10$, and perform **five steps** of gradient descent using a learning rate of $\\delta=0.3$ (there is no principled reason for us picking this particular value for $\\delta$ - it simply suits this example well; more on selecting learning rate later).\n",
    "\n",
    "\\begin{align}\n",
    "w_\\mathrm{new} &= w_\\mathrm{old} - \\delta \\frac{\\mathrm{d}\\mathscr{L}}{\\mathrm{d}w}\\bigg|_{w=w_\\mathrm{old}}\\\\\n",
    "w_\\mathrm{new} &= w_\\mathrm{old} - (0.3) (2 w_\\mathrm{old})\\\\\n",
    "&\\Downarrow \\\\\n",
    "4 &= 10 - (0.3) (2 \\times 10)\\\\\n",
    "1.6 &= 4 - (0.3) (2 \\times 4)\\\\\n",
    "0.64 &= 1.6 - (0.3) (2 \\times 1.6)\\\\\n",
    "0.256 &= 0.64 - (0.3) (2 \\times 0.64 )\\\\\n",
    "0.1024 &= 0.256 - (0.3) (2 \\times 0.256)\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that the sequence of $w$ values indeed corresponds to a \"descent\" down $\\mathscr{L}(w)$, towards $w=0$."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<div style=\"text-align: center\">\n",
    "<p>\n",
    "<img src=\"../_images/parabola_descent.png\" alt=\"Depicting gradient descent down a parabola\" width=\"500\">\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that even though the learning rate, $\\delta=0.3$, is constant, the distance between subsequent $w$-values gets shorter as they near $w_\\mathrm{min}$;\n",
    "this is due to the fact that $\\frac{\\mathrm{d}\\mathscr{L}}{\\mathrm{d}w}$, which multiplies $\\delta$, naturally shrinks in magnitude near the minimum (i.e. the slope approaches zero), and thus the update to $w_\\mathrm{old}$ becomes more \"refined\" in its vicinity.\n",
    "\n",
    "We stopped the gradient descent process where we did out of sheer convenience; \n",
    "we don't usually carry our gradient descent by hand – we will have a computer do the hard work for us!\n",
    "The terminal value of $w_\\mathrm{stop} = 0.1024$ isn't especially close to the true minimum of the parabola, but we cannot make a decision about \"how close is close enough\" in a principled way without having additional context about _why_ we want to find this minimum.\n",
    "In practice, we will need some additional metric of success to inform how close is \"close enough\" when we are searching for the minimum of a function.\n",
    "For our present purposes, this example simply demonstrates the mechanics of and logic behind gradient descent.\n",
    "The following two reading comprehension questions are particularly important for cementing these takeaways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Reading Comprehension: Descent Down a Parabola in Python**:\n",
    "\n",
    "Complete the following Python function that implements gradient descent on $\\mathscr{L}(w) = w^2$.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def descent_down_parabola(w_start, learning_rate, num_steps):\n",
    "    \"\"\"\n",
    "    Performs gradient descent on L(w) = w ** 2, returning the sequence\n",
    "    of x-values: [w_start, ..., w_stop]\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    w_start : float\n",
    "        The initial value of w.\n",
    "    \n",
    "    learning_rate : float\n",
    "        The \"learning rate\" factor for each descent step. A positive number.\n",
    "    \n",
    "    num_steps : int\n",
    "        The number subsequent of descent steps taken. A non-negative number.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray, shape-(num_steps + 1, )\n",
    "        The sequence of w-values produced via gradient descent, starting with w_start\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "```    \n",
    "\n",
    "Test your function using the inputs `w_start=10`, `learning_rate=0.3`, and `num_steps=5` and confirm that your function reproduces the results presented in this discussion.\n",
    "Repeat this computation using `w_start=-10`, and see that gradient descent reliably coaxes $w$ in the opposite direction – still towards the global minimum of the parabola.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence\n",
    "\n",
    "Studying the equation for gradient descent for a single-variable function, when will the updates that it yields \"converge\" to an answer? That is, when will our update say $w_\\mathrm{new} \\approx w_\\mathrm{old}$?\n",
    "This will occur when $\\delta \\frac{\\mathrm{d}\\mathscr{L}}{\\mathrm{d}w} \\approx 0$, which is to say that this will occur whenever we reach a plateau in out loss landscape. Determining just how flat a region of a function must be for it to be considered a plateau ultimately depends on the size of $\\delta$.\n",
    "\n",
    "At a plateau, the process of gradient descent will have **converged** upon some terminal value – further iterations via gradient descent will only yield inconsequentially-small updates to this terminus.\n",
    "If the function has no such flat region, then the gradient descent algorithm will produce perpetual revisions to $w_\\mathrm{new}$ that descend further into the abyss of negative infinity.\n",
    "\n",
    "The following image depicts three varieties of plateaus: a maximum, a saddle point, and a minimum. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<div style=\"text-align: center\">\n",
    "<p>\n",
    "<img src=\"../_images/plateaus.png\" alt=\"Depicting the types of plateaus that can manifest in a single-variable function\" width=\"900\">\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that these are all places (marked by the red x's) where the slope of the function of interest is $0$; thus there is some neighborhood about each of these points for which $\\delta \\frac{\\mathrm{d}\\mathscr{L}}{\\mathrm{d}w} \\approx 0$.\n",
    "Gradient descent should never lead us \"up\" a function, so the local maximum is not a feature that we will need to worry about, but saddle points and local minima are certainly places where this algorithm can converge to.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Reading Comprehension: Analyzing Descent Convergence**:\n",
    "\n",
    "Using your implementation of `descent_down_parabola`, and the inputs `w_start=10`, `learning_rate=0.3`, and `num_steps=100`, [use matplotlib](https://www.pythonlikeyoumeanit.com/Module5_OddsAndEnds/Matplotlib.html) to make a plot of \"Distance of $w$ from Minimum vs Number of Steps\".\n",
    "You should see that $w$ approaches $0$ so rapidly that it is hard to discern its trajectory on a linear scale;\n",
    "try plotting the $y$-axis on a log scale (you can use `ax.set_yscale(\"log\")`).\n",
    "\n",
    "Describe the mathematical form of the trajectory of $w$ towards the minimum.\n",
    "Does the process of gradient descent ever lead $w$ away from the minimum?\n",
    "Try experimenting with different learning rates before you come to a final conclusion.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent on Multi-Variable Functions\n",
    "\n",
    "The utility of gradient descent might seem questionable when we only consider functions of one variable.\n",
    "In these cases, it seems like we can simply plot such a function and locate minima by eye!\n",
    "Let's remember, however, that we are going to want to employ gradient descent to obtain optimal values for the parameters of sophisticated mathematical models used in the context of machine learning.\n",
    "In the case of our simple linear regression problem, this means working with a function of two variables.\n",
    "That being said, once we graduate to working with neural networks, we will find ourselves soon working with mathematical models represented by functions of _hundreds, thousands, millions,_ [or even billions](https://en.wikipedia.org/wiki/GPT-3) of variables!\n",
    "\n",
    "Needless to say, once we are working with functions of several variables, we will have no hope of visualizing our function so as to simply locate its minima by eye.\n",
    "Instead, we will be like [a mountain climber trying to descend a mountain covered in a thick fog](https://en.wikipedia.org/wiki/Gradient_descent#An_analogy_for_understanding_gradient_descent): we can only attempt to find our way down the mountain by feeling the way that the ground slopes under our feet and systematically stepping in the direction that slopes most downward.\n",
    "(Note that this process of \"feeling the slope of the mountain\" is an analogy for evaluating the [gradient](https://rsokl.github.io/CogWeb/Math_Materials/Multivariable_Calculus.html#Taking-a-Derivative-Along-Any-Direction-Using-the-Gradient) of our multi-variable function; it reveals only the local topography of the mountain at a particular point in space.)\n",
    "\n",
    "Ultimately, gradient descent is a process by which we can find a minimum of a multi-variable function.\n",
    "E.g. if we were working with a function of three variables, $\\mathscr{L}(w_1, w_2, w_3)$, then gradient descent would permit us to search for the point $(w_1, w_2, w_3)_\\mathrm{min}$ that corresponds to a local minimum of $\\mathscr{L}$.\n",
    "\n",
    "Fortunately, the general equation for the gradient step of multiple variables is quite straightforward.\n",
    "Suppose that we are performing gradient descent  with a function of $M$ variables, $\\mathscr{L}(w_1, ..., w_M)$.\n",
    "Then **the set of M equations describing a single iteration of gradient descent on a function of M variables is as follows**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\begin{bmatrix} w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_M \\end{bmatrix}_\\text{new} = \\begin{bmatrix} w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_M \\end{bmatrix}_\\text{old} - \\delta\\: \\begin{bmatrix} \\frac{\\partial \\mathscr{L}}{\\partial w_1} \\\\ \\frac{\\partial \\mathscr{L}}{\\partial w_2} \\\\ \\vdots \\\\ \\frac{\\partial \\mathscr{L}}{\\partial w_M} \\end{bmatrix}_\\text{old}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, $\\delta$ plays the role of the so-called \"learning rate\".\n",
    "Note that the same learning rate value is used across each of the $M$ dimensions here.\n",
    "\n",
    "To reiterate, the single equation above specifies $M$ *separate equations* – each one is a \"recipe\" for updating the value of a corresponding variable in search for a minimum of $\\mathscr{L}$.\n",
    "We invoke [vector notation](https://rsokl.github.io/CogWeb/Math_Materials/LinearAlgebra.html) to more concisely express these \"parallel\" equations.\n",
    "Let's focus on the update being made to the $j^\\text{th}$ variable to see just how closely it resembles the form for gradient descent of a single-variable function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "w_{j}^{\\mathrm{new}} = w_{j}^{\\mathrm{old}} - \\delta\\:\\Bigg( \\frac{\\partial \\mathscr{L}(w_1, ..., w_M)}{\\partial w_j}\\bigg|_{w_1=w_{1}^{\\mathrm{old}} \\dots w_M=w_{M}^{\\mathrm{old}}}\\Bigg)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that we utilize the [partial derivative](https://rsokl.github.io/CogWeb/Math_Materials/Multivariable_Calculus.html#What-is-a-Partial-Derivative?) of $\\mathscr{L}$ with respect to $w_j$ here, which measures the instantaneous slope of $\\mathscr{L}$ along $w_j$, holding all other variables fixed.\n",
    "\n",
    "Mechanically speaking, we we simply replaced the derivative of $\\mathscr{L}(w)$ with a partial derivative, and replicated the equation in correspondence with each of the $M$ variables of $\\mathscr{L}$; that being said, this generalization is not made haphazardly.\n",
    "We will see that this is indeed the _optimal_ formula for searching for a minimum of $\\mathscr{L}$ if we are only to use its first-order derivatives.\n",
    "Towards this end, it is critical to note that the collection of partial derivatives $\\begin{bmatrix} \\frac{\\partial \\mathscr{L}}{\\partial w_1} & \\cdots & \\frac{\\partial \\mathscr{L}}{\\partial w_M} \\end{bmatrix}$ is far more special than it might appear;\n",
    "this is, in fact, **the gradient of** $\\mathscr{L}(w_1, ..., w_M)$\n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{\\nabla} \\mathscr{L}(w_1, w_2, \\dots, w_M)=\\begin{bmatrix} \\frac{\\partial \\mathscr{L}}{\\partial w_1} & \\frac{\\partial \\mathscr{L}}{\\partial w_2} & \\cdots & \\frac{\\partial \\mathscr{L}}{\\partial w_M} \\end{bmatrix}.\n",
    "\\end{equation}\n",
    "\n",
    "The gradient of $\\mathscr{L}$, $\\vec{\\nabla} \\mathscr{L}$, is a vector-field that has the special property that it [always points in the direction of steepest ascent](https://rsokl.github.io/CogWeb/Math_Materials/Multivariable_Calculus.html#Properties-of-the-Gradient) on $\\mathscr{L}$ wherever it is evaluated (it is worthwhile to review [our content on linear algebra](https://rsokl.github.io/CogWeb/Math_Materials/LinearAlgebra.html) and [multivariable calculus](https://rsokl.github.io/CogWeb/Math_Materials/Multivariable_Calculus.html#) to better digest this).\n",
    "This means that $\\vec{\\nabla} \\mathscr{L}(\\vec{w}_{\\mathrm{old}})$ provides us with critically-important information: it tells us, based on our current model parameter values ($\\vec{w}_{\\mathrm{old}}$) and our knowledge of its first-order derivatives at this point, the _best_ incremental update that we can make to our model's parameters in order to minimize $\\mathscr{L}$.\n",
    "In the same way that an individual derivative describes the slope of a function along a single direction, the gradient of a function indicates how that function changes along multiple dimensions.\n",
    "\n",
    "Rewriting the gradient update using this gradient notation, and representing $(w_1, \\dots, w_M)$ as the vector $\\vec{w}$, we have\n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{w}_\\mathrm{new} = \\vec{w}_\\mathrm{old} - \\delta \\, \\vec{\\nabla} \\mathscr{L}(w_1, \\dots, w_M)\\big|_{\\vec{w}=\\vec{w}_\\mathrm{old}}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Thus the equation of gradient descent indicates that $\\vec{w}_\\mathrm{old}$ is always \"nudged\" in the direction of steepest _descent_ of $\\mathscr{L}$, as dictated by $-\\vec{\\nabla} \\mathscr{L}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Looking Ahead: Automatic Differentiation**\n",
    "\n",
    "The sorts of mathematical models that are used to in deep learning represent functions of **many** variables (thousands of variables would often times be be on the _low_ end!) – so many that it would be hopeless for us to derive all of the partial derivatives, $\\begin{bmatrix} \\frac{\\partial \\mathscr{L}}{\\partial w_1} & \\frac{\\partial \\mathscr{L}}{\\partial w_2} & \\cdots & \\frac{\\partial \\mathscr{L}}{\\partial w_M} \\end{bmatrix}$, by hand in order to perform gradient descent!\n",
    "\n",
    "Fortunately, many powerful and easy-to-use software libraries have emerged that are designed to perform **automatic differentiation**.\n",
    "An automatic differentiation library provides one with a wide array of tools for performing standard mathematical calculations (similar to using NumPy), but these tools are designed so that you can use them to compute the outputs of large functions _and_ to have them evaluate the function's derivatives!\n",
    "In this way you need not write a single derivative yourself.\n",
    "So-called \"autodiff\" libraries are thus absolutely essential tools for conducting any sort of gradient-based optimizations on sophisticated mathematical models.\n",
    "\n",
    "In the next section, we will familiarize ourselves with the simple automatic differentiation library known as [MyGrad](https://mygrad.readthedocs.io/en/latest/).\n",
    "This library was designed first and foremost as an educational tool; in fact, it was originally developed specifically for CogWorks!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descending a Parabaloid Using Gradient Descent\n",
    "\n",
    "As always, it is important that we make concrete the mathematical foundation that we are laying by working through an example.\n",
    "Let's descend the parabaloid surface\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathscr{L}(w_1, w_2) = 2 w_1^2 + 3 w_2^2\n",
    "\\end{equation}\n",
    "\n",
    "First, we need to [compute the gradient](https://rsokl.github.io/CogWeb/Math_Materials/Multivariable_Calculus.html#Taking-a-Derivative-Along-Any-Direction-Using-the-Gradient) for $\\mathscr{L}(w_1, w_2)$ by writing out its [partial derivatives](https://rsokl.github.io/CogWeb/Math_Materials/Multivariable_Calculus.html#What-is-a-Partial-Derivative?)\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial\\mathscr{L}}{\\partial w_1} = 4 w_1\\\\\\\n",
    "\\frac{\\partial\\mathscr{L}}{\\partial w_2} = 6 w_2\\\\\\\n",
    "\\end{equation}\n",
    "\n",
    "Thus the gradient of $\\mathscr{L}$ is given by the following vector field\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{\\nabla} \\mathscr{L}(w_1, w_2) = \\begin{bmatrix} \\frac{\\partial \\mathscr{L}}{\\partial w_1} & \\frac{\\partial \\mathscr{L}}{\\partial w_2} \\end{bmatrix} = [4 w_1, 6 w_2]\n",
    "\\end{equation}\n",
    "\n",
    "which is simple to evaluate at any point $(w_1, w_2)$ during our descent.\n",
    "\n",
    "(Note: it just happens to be the case that $\\frac{\\partial\\mathscr{L}}{\\partial w_1}$ only depends on $w_1$ and $\\frac{\\partial\\mathscr{L}}{\\partial w_2}$ only depends on $w_2$ here.\n",
    "In general, each partial derivative of $\\mathscr{L}$ could depend on any/all of the model's parameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with $\\vec{w}_\\mathrm{old} = [2, 4]$ and using a learning rate of $\\delta=0.1$.\n",
    "We'll take five steps down $\\mathscr{L}$ using gradient descent.\n",
    "\n",
    "(For the sake of legibility, we are rounding to two decimal places)\n",
    "\n",
    "\\begin{align}\n",
    "\\vec{w}_\\mathrm{new} &= \\vec{w}_\\mathrm{old} - \\delta \\vec{\\nabla} \\mathscr{L}|_{\\vec{w}=\\vec{w}_\\mathrm{old}}\\\\\n",
    "\\vec{w}_\\mathrm{new} &= \\vec{w}_\\mathrm{old} - \\delta \\begin{bmatrix} \\frac{\\partial \\mathscr{L}}{\\partial w_1}|_{\\vec{w}=\\vec{w}_\\mathrm{old}} & \\frac{\\partial \\mathscr{L}}{\\partial w_2}|_{\\vec{w}=\\vec{w}_\\mathrm{old}} \\end{bmatrix}\\\\\n",
    "\\vec{w}_\\mathrm{new} &= [w^{\\mathrm{(old)}}_1, w^{\\mathrm{(old)}}_2] - (0.1) [4 w^{\\mathrm{(old)}}_1, 6 w^{\\mathrm{(old)}}_2]\\\\\n",
    "&\\Downarrow \\\\\n",
    "[1.20    , 1.60    ] &= [2.00, 4.00] - (0.1) [8.00, 24.0]\\\\\n",
    "[0.72   , 0.64   ] &= [1.20    , 1.60    ] - (0.1) [ 4.8    ,  9.6    ]\\\\\n",
    "[0.43  , 0.26  ] &= [0.72   , 0.64   ] - (0.1) [ 2.88   ,  3.84   ]\\\\\n",
    "[0.26 , 0.10] &= [0.43  , 0.26  ] - (0.1) [ 1.73  ,  1.54  ]\\\\\n",
    "[0.16, 0.04] &= [0.26 , 0.10] - (0.1) [ 1.04 ,  0.61 ]\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following figure depicts this paraboloid, along with the five-step trajectory that we just calculated."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<div style=\"text-align: center\">\n",
    "<p>\n",
    "<img src=\"../_images/paraboloid_descent.png\" alt=\"Depicting gradient descent down a paraboloid\" width=\"1000\">\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, the iterative process of gradient descent guides us towards parameter values towards the minimum of $\\mathscr{L}$, which resides at $(0, 0)$; however this time we invoke the gradient of a multivariable function instead of the derivative of a single variable function, so that we can search for the minimum within a higher dimensional space.\n",
    "And to reiterate our previous discussion: whether our last \"best guess\" at a minimum – $(0.16, 0.04)$ – is \"good enough\" would depend on the broader context of _why_ we want to find this minimum.\n",
    "In the context of a machine learning problem, \"good enough\" might be informed by how consistently our model, whose parameters are given by this best guess, makes predictions that are in agreement with collected data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Reading Comprehension: Descent Down a Parabolic Surface in Python**:\n",
    "\n",
    "Complete the following Python function that implements gradient descent on the skewed paraboloid $\\mathscr{L}(w_1, w_2) = 2 w_1^2 + 3 w_2^2$.\n",
    "\n",
    "Note that the partial derivatives of this function are simply\n",
    "    \n",
    "\\begin{equation}\n",
    "\\frac{\\partial \\mathscr{L}(w_1, w_2)}{\\partial x} = 4 w_1 \\\\\n",
    "\\frac{\\partial \\mathscr{L}(w_1, w_2)}{\\partial y} = 6 w_2 \\\\\n",
    "\\end{equation}\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def descent_down_2d_parabola(w_start, learning_rate, num_steps):\n",
    "    \"\"\"\n",
    "    Performs gradient descent on L(w1, w2) = 2 * w1 ** 2 + 3 * w2 **2 , \n",
    "    returning the sequence of w-values: [w_start, ..., w_stop]\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    w_start : np.ndarray, shape-(2,)\n",
    "        The initial value of (w1, w2).\n",
    "\n",
    "    learning_rate : float\n",
    "        The \"learning rate\" factor for each descent step. A positive number.\n",
    "\n",
    "    num_steps : int\n",
    "        The number subsequent of descent steps taken. A non-negative number.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray, shape-(num_steps + 1, 2)\n",
    "        The sequence of (w1, w2)-values produced via gradient descent, starting \n",
    "        with w_start\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "```    \n",
    "\n",
    "Test your function using the inputs `w_start=np.array([2.0, 4.0])`, `learning_rate=0.1`, and `num_steps=5` and, by hand, confirm that your function correctly performed the first gradient step.\n",
    "Repeat this computation using `xy_start=np.array([-2.0,-4.0])`, and see that gradient descent reliably coaxes $\\vec{w}$ in the opposite direction - still towards the global minimum of the paraboloid.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## The \"Hyperparameters\" of Gradient Descent\n",
    "\n",
    "Reflect on your implementation of `descent_down_2d_parabola` from the reading comprehension problem above.\n",
    "Assuming that the specific function being optimized by gradient descent is fixed, what are the pieces of information that are needed to specify the rest of the implementation of the algorithm?\n",
    "\n",
    "There are three configurable aspects to this \"vanilla\" implementation of gradient descent (there are variations of gradient descent that include more configurable aspects):\n",
    "\n",
    "1. The method by which we choose the initial values for $\\vec{w}_{\\mathrm{old}}$.\n",
    "2. The value of the learning rate, $\\delta$.\n",
    "3. The criterion for ending the algorithm (i.e. determining when the computer should stop computing values for $\\vec{w}_{\\mathrm{new}}$).\n",
    "\n",
    "\n",
    "these are often referred to as the **hyperparameters** of gradient descent.\n",
    "\n",
    "The term **\"hyperparameter\" is used to distinguish a parameter that is used to configure an aspect of a machine learning algorithm**, whereas the term **\"parameter\" is used to refer to a value that is subject to being updated by the machine learning algorithm itself.**\n",
    "Thus $\\vec{w}$ is a vector of *parameters*, which are revised by the gradient descent algorithm, whereas $\\delta$ is a *hyperparameter* whose value must be determined independently as a pre-condition of using gradient descent.\n",
    "Note that a hyperparameter need not always refer to a numerical value, as it does in the case of $\\delta$; neither the initialization scheme nor termination criterion for gradient descent can be represented by a single number, but their configurations are nonetheless necessary for implementing gradient descent and thus they are deemed to be hyperparameters. \n",
    "\n",
    "\n",
    "So how do we go about selecting these hyperparameters?\n",
    "The answer invariably is: _it depends_.\n",
    "It depends on the problem that we are interested in solving, the specifics of the mathematical model whose parameters we are tuning, and what our criterion for \"success\" is.\n",
    "For sufficiently simple or well-behaved functions (like a convex function), there are rigorous approaches to choosing these hyperparameters in a way that provides nice theoretical guarantees for our results (e.g. \"using these specific hyperparameters, you are guaranteed to be within some concrete $\\epsilon$ of the function's global minimum\").\n",
    "However, neural networks are not amenable to these rigorously-founded guidelines.\n",
    "Rather, **the means by which the hyperparameters are chosen for neural networks is more a matter of practiced art than it is a consequence of mature theoretical foundations**.\n",
    "(Hopefully this will not be the case for long, and that theoretical results will be developed to help inform this process for neural networks.)\n",
    "\n",
    "Let's discuss some of the common, high-level strategies for choosing these hyperparameters, with the understanding that we will fill details as we apply gradient descent on concrete machine learning problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing A Parameter Initialization Scheme\n",
    "\n",
    "Given our mathematical model, $F\\big((w_1, ..., w_M); (x_n)_{n=0}^{N-1}\\big)$, where $(x_n)_{n=0}^{N-1}$ represents our collected input data, we need to choose some definite values for the model's parameters $(w_1, ..., w_M)$ so that the model can be used to produce concrete results, from which we can evaluate a loss, $\\mathscr{L}$, and then use gradient descent to refine these parameters. \n",
    "**Often times we will draw these initial values from random statistical distributions**, such as a Gaussian distribution.\n",
    "\n",
    "In the context of deep learning, where a neural network is used as our mathematical model, there are [a handful of popular initialization techniques](https://mygrad.readthedocs.io/en/latest/nnet.html#initializers) that are derived based on some rough assumptions about the distribution of data, $(x_n)_{n=0}^{N-1}$, that one is working with as well as the so-called \"activation functions\" one uses within the neural network (more on this later).\n",
    "\n",
    "Over the past several years, popular neural network architectures and regularizations schemes have emerged that serve to greatly diminish the importance of any particular initialization scheme.\n",
    "This wasn't always the case; methods used in the early-to-mid 2010's to construct neural networks were such that the particular initialization scheme used could make all the difference between a model that could be deftly tuned by gradient descent and one for which gradient descent made no useful progress at all. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing A Learning Rate\n",
    "\n",
    "Choosing a suitable learning rate for your problem is very important; using a $\\delta$ that is too big can cause your parameter values to update too severely, which can cause the desirable minimum in the loss landscape to be \"leapt\" over.\n",
    "A learning rate can be so large as to make your model's performance diverge to an ever-worsening state.\n",
    "On the other hand, choosing a very small learning rate can guarantee that our model will never diverge and that a landscape minimum will never be leapt over, but it also can mean that the progress that we make in tuning our model is very slow, and that we will be trapped by intermediate and non-ideal plateaus in the loss landscape.\n",
    "\n",
    "In practice, we can often simply refer to other work that uses gradient descent on a model relevant to our own to get a sense of what an appropriate learning rate value might be.\n",
    "That being said, it is still useful to consider how we might try to find a good value on our own.\n",
    "\n",
    "First, we want to find the appropriate **order of magnitude** (i.e. multiple of ten) for our learning rate.\n",
    "A desirable learning rate is one that prompts a reliable descent down our loss landscape's minima in a \"reasonable\" number of steps (i.e. not so many that it is prohibitively expensive).\n",
    "\n",
    "Let's return to the problem of performing gradient descent down $L(w) = w^2$.\n",
    "The following plots compares the performance of gradient descent across five orders of magnitude of learning rates.\n",
    "Each run starts at $w_{\\mathrm{old}}=100$ and lasts for $1,000$ iterations. \n",
    "(Note that these plot can be easily be reproduced by making a simple modification to the solution of the \"Analyzing Descent Convergence\" reading comprehension problem. In essence: put the solution in a for-loop over learning rates.)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<div style=\"text-align: center\">\n",
    "<p>\n",
    "<img src=\"../_images/grad_desc_order_of_mag_analysis.png\" alt=\"Comparison of learning rates varied by order of magnitude\" width=\"1000\">\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that the quality of our results – **how close** we get to the minimum of our function, and **how quickly** we get there – varies wildly across the orders of magnitudes of learning rates that we compare here.\n",
    "\n",
    "The learning rate $\\delta = 0.1$ is clearly the winning pick in this example;\n",
    "the smaller learning rates eventually lead to a comparable final solution, but they do so *much* more slowly.\n",
    "The next-largest learning rate takes ten times as long to descend the loss landscape (which makes sense if you consider that we are taking steps that are roughly one tenth as large at this rate).\n",
    "\n",
    "That being said, we have certainly found a sweet spot;\n",
    "increasing the learning rate to $\\delta = 0.999$ appears greatly deteriorate the rate of convergence;\n",
    "here we are leaping back and forth across the parabola's minimum, but _slowly_ making our way to the minimum as we do so.\n",
    "Bumping the learning rate above $\\delta = 1.0$ leads to a diverging trajectory.\n",
    "\n",
    "While this example is an overly-simplistic one, it teaches an important lesson that holds quite broadly: picking the right order of magnitude for your learning rate can make all of the difference when optimizing the parameters of your model.\n",
    "A model can be made to look incapable of \"learning\" a problem if this hyperparameter is poorly chosen.\n",
    "\n",
    "Once you are in the ballpark of the correct order of magnitude for your learning rate, you can do a finer-grained search within that neighborhood of learning rates.\n",
    "This too can reveal a niche of improved performance (although it must be noted that more realistic problems – ones that don't involve optimizing over a very nice, convex function – will likely not exhibit such dramatic differences at this level).\n",
    "Note that it can be beneficial to do a random sampling over this small region of learning rates rather than a uniform, grid-based sampling as we have done here."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<div style=\"text-align: center\">\n",
    "<p>\n",
    "<img src=\"../_images/grad_desc_fine_tuned_analysis.png\" alt=\"Comparison of learning rates varied by order of magnitude\" width=\"1000\">\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is plenty more to say about performing [hyperparameter tuning](https://scikit-learn.org/stable/modules/grid_search.html), and there are particularly important remarks to be made about using methods [like cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html) to avoid tuning hyperparameters to become \"overfit\" to the particular data, $(x_n, y_n)_{n=0}^{N-1}$, that we are feeding our model.\n",
    "Additionally, the learning rate can be adjusted during gradient descent.\n",
    "For example, you can decrease the learning rate by a constant factor every $N$ steps, in hopes that you will take \"more discerning\" steps as you get closer to the desirable minima of the loss landscape.\n",
    "This is broadly referred to as a \"learning rate schedule\".\n",
    "\n",
    "That being said, these topics are beyond the scope of this already-burgeoning sub-sub-section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a Stopping Condition\n",
    "\n",
    "We generally end gradient descent (or perhaps change our learning rate) when it looks like we have reached a plateau in our function.\n",
    "In the plot above - comparing gradient descent down a parabola using different learning rates - we can see such plateaus manifest.\n",
    "The red curve (the one that makes the fastest descent) indicates that our search for the global minimum started near $\\mathscr{L}(w_\\mathrm{new})=80$  (it actually started at $100$, but changed so quickly that the plot couldn't capture it) and quickly plumetted to $\\mathscr{L}(w_\\mathrm{new})\\approx 0$.\n",
    "At this point we would need to change the scale of our $\\mathscr{L}$ axis to see if we are making any progress, but based on our initial scale we certainly no longer see meaningful progress.\n",
    "\n",
    "Often times we can use [visualization tools](https://noggin.readthedocs.io/en/latest/) that plot $\\mathscr{L}(w_\\mathrm{new})$ in real time, and we can **manually stop the process of gradient descent** when we see things flatten out this way.\n",
    "While this isn't a glamarous nor a principled stopping condition, it is a very common practice.\n",
    "In this scheme, we might tell gradient descent to proceed for a very large number of steps with the expectation that some plateau will be reached well before the formal stopping condition is reached.\n",
    "\n",
    "An **autonomous stopping condition** is one where we specify some heuristic condition for detecting when a plateau has been reached, and tell the gradient descent algorithm to halt if this condition has been met.\n",
    "For example, we might keep track of, say, the last $50$ values of $\\mathscr{L}(w_\\mathrm{new})$ from our descent and fit a line on $\\mathscr{L}$ vs $N_{\\mathrm{steps}}$: if the slope of the line is sufficiently shallow then our algorithm could detect we have stopped making substantial progress in our descent and thus end the process.\n",
    "Note that this is just a simple example of how one might automate a stopping condition, and that there are better and more sophisticated approaches to doing this that one should consider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Given a function $\\mathscr{L}(w_1, ..., w_M)$, suppose that we want to find a collection of values for $(w_1, ..., w_M)$ that *minimizes* $\\mathscr{L}$.\n",
    "Assuming that $\\mathscr{L}$ is smooth and differentiable, we can search for these parameter values that minimize $\\mathscr{L}$ using a process known as gradient descent.\n",
    "Representing $(w_1, ..., w_M)$ as the vector $\\vec{w}$, we suppose that we have some initial values for these parameters, $\\vec{w}_{\\mathrm{old}}$, where we begin out search for them minimizing parameter values.\n",
    "Then gradient descent prescribes the following _iterative_ process by which we update our \"best guess\" for these values:\n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{w}_\\mathrm{new} = \\vec{w}_\\mathrm{old} - \\delta \\, \\vec{\\nabla} \\mathscr{L}(w_1, \\dots, w_M)\\big|_{\\vec{w}=\\vec{w}_\\mathrm{old}}\\\\\n",
    "\\vec{w}_\\mathrm{old} \\leftarrow \\vec{w}_\\mathrm{new} \\\\\n",
    "\\mathrm{(repeat)}\n",
    "\\end{equation}\n",
    "\n",
    "$\\delta$ is a constant, positive number that we are responsible for choosing.\n",
    "This is often times referred to as a \"learning rate\", as it affects the scale of the updates that we make to $\\vec{w}_\\mathrm{old}$.\n",
    "\n",
    "The key insight to this process is that the gradient of $\\mathscr{L}$, $\\vec{\\nabla} \\mathscr{L}$, evaluated at any point in this $M$-dimensional space, spanned by $(w_1, ..., w_M)$, **points in the direction of steepest ascent** for $\\mathscr{L}$.\n",
    "To think of ourselves as a mountain climber trying to descend a mountain in a thick fog, the evaluated gradient is analogous to our feeling the slope of the mountain immediately beneath our feet - it lets us figure out which direction we should step in to try to descend the mountain.\n",
    "Thus the iteration laid out above prescribes that we repeatedly update $\\vec{w}_\\mathrm{old}$ by nudging it in the direction _opposite_ to the gradient, along the direction of steepest _descent_. Given a sufficiently small $\\delta$, these updates will eventually converge to a local plateau of $\\mathscr{L}$, where $\\vec{\\nabla} \\mathscr{L}\\big|_{\\vec{w}=\\vec{w}_\\mathrm{old}} \\approx \\vec{0}$.\n",
    "Our hope is that this plateau resides specifically at the global minimum of $\\mathscr{L}$.\n",
    "\n",
    "In the context of machine learning, $\\mathscr{L}$ typically represents a so-called \"loss function\" (or objective function), $(w_1, ..., w_M)$ represent our model's parameters, and $\\mathscr{L}$ also depends on our collected data, which we hold as constant here.\n",
    "$\\mathscr{L}$ will be responsible for comparing our model's predictions about recorded observations to the desired, or \"true\", predictions that we want it to make.\n",
    "We design $\\mathscr{L}$ such that _better predictions from our model will produce smaller loss values_.\n",
    "Thus we can use gradient descent to search for particular values of $(w_1, ..., w_M)$ that minimizes $\\mathscr{L}$, and thereby we arrive at the parameter values that enable our model to make the most accurate and reliable predictions about our recorded data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Comprehension Exercise Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Retracing Our Steps: Solution**\n",
    "\n",
    "Given a simple linear model\n",
    "\n",
    "\\begin{equation}\n",
    "F\\big((w_1, w_2); x \\big) = w_2 x + w_1\n",
    "\\end{equation}\n",
    "\n",
    "Assuming that we have $N$ pieces of recorded observations and associated \"true\" outcomes, $(x_n, y_n)_{n=0}^{N-1}$, **write the \"mean-squared error\" loss function** in terms of the model's parameters, and the individual pieces of data.\n",
    "\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\mathscr{L}_{\\mathrm{MSE}}\\big(w_1, w_2 ; (x_n, y_n)_{n=0}^{N-1}\\big) &= \\frac{1}{N}\\sum_{n=0}^{N-1}{\\big(y_n - y^{\\mathrm{(pred)}}_n\\big)^2}\\\\\n",
    "&= \\frac{1}{N}\\sum_{n=0}^{N-1}{\\big(y_n - F\\big((w_1, w_2); x_n \\big)\\big)^2}\\\\\n",
    "&= \\frac{1}{N}\\sum_{n=0}^{N-1}{\\big(y_n - (w_2 x_n + w_1)\\big)^2}\\\\\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Descent Down a Parabola in Python: Solution**:\n",
    "\n",
    "Complete the following function that implements gradient descent on $\\mathscr{L}(w) = w^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def descent_down_parabola(w_start, learning_rate, num_steps):\n",
    "    \"\"\"\n",
    "    Performs gradient descent on L(w) = w ** 2, returning the sequence\n",
    "    of x-values: [w_start, ..., w_stop]\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    w_start : float\n",
    "        The initial value of w.\n",
    "    \n",
    "    learning_rate : float\n",
    "        The \"learning rate\" factor for each descent step. A positive number.\n",
    "    \n",
    "    num_steps : int\n",
    "        The number subsequent of descent steps taken. A non-negative number.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray, shape-(num_steps + 1, )\n",
    "        The sequence of w-values produced via gradient descent, starting with w_start\n",
    "    \"\"\"\n",
    "    w_values = [w_start]\n",
    "    for _ in range(num_steps):\n",
    "        w_old = w_values[-1]\n",
    "        w_new = w_old - learning_rate * (2 * w_old)\n",
    "        w_values.append(w_new)\n",
    "    return np.array(w_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your function using the inputs `w_start=10`, `learning_rate=0.3`, and `num_steps=5` and confirm that your function reproduces the results presented in this discussion.\n",
    "\n",
    "```python\n",
    ">>> descent_down_parabola(10, 0.3, 5)\n",
    "array([10.    ,  4.    ,  1.6   ,  0.64  ,  0.256 ,  0.1024])\n",
    "```\n",
    "\n",
    "Repeat this computation using `w_start=-10`, and see that gradient descent reliably coaxes $w$ in the opposite direction - still towards the global minimum of the parabola.\n",
    "\n",
    "```python\n",
    ">>> descent_down_parabola(-10, 0.3, 5)\n",
    "array([-10.    ,  -4.    ,  -1.6   ,  -0.64  ,  -0.256 ,  -0.1024])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyzing Descent Convergence : Solution**:\n",
    "\n",
    "Using your implementation of `descent_down_parabola`, using the inputs `w_start=10`, `learning_rate=0.3`, and `num_steps=100`, [use Matplotlib](https://www.pythonlikeyoumeanit.com/Module5_OddsAndEnds/Matplotlib.html) to make a plot of \"Distance of $w$ from Minimum vs Number of Steps\".\n",
    "You should see that $w$ approaches $0$ so rapidly that it is hard to discern its trajectory on a linear scale;\n",
    "try plotting the $y$-axis on a log scale (you can use `ax.set_yscale(\"log\")`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "out = descent_down_parabola(w_start=10, learning_rate=0.3, num_steps=100)\n",
    "ax.plot(np.abs(out))\n",
    "ax.set_yscale(\"log\")\n",
    "ax.grid(True)\n",
    "\n",
    "ax.set_ylabel(\"Distance from Minimum\")\n",
    "ax.set_xlabel(\"Number of Steps\")\n",
    "ax.set_title(r\"Analyzing Convergence of Gradient Descent on $\\mathscr{L}(w)=w^2$\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the mathematical form of the trajectory of $w$ towards the minimum.\n",
    "\n",
    "> We see $w$ approaches ever closer to the true minimum of $w^2$ via the iterative process of gradient descent. The distance between $w$ and the minimum ($0$) decreases exponentially quickly with number of descent steps. This is apparent from the linearly-decreasing form of the distance-from-the-minimum versus number of steps, **viewed on a logarithmic scale for the y-axis**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the process of gradient descent ever lead $w$ away from the minimum?\n",
    "Try experimenting with different learning rates before you come to a final conclusion.\n",
    "\n",
    "> Increasing the learning rate can lead to updates to $w$ that are so large that we end up inadvertently _ascending_ the parabola.\n",
    "> That is, each step results in us leaping from one side of the parabola to the other, and further away from the minimum than we were before.\n",
    "> See this below, where we increase the learning rate from $0.3$ to $3.0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "out = descent_down_parabola(w_start=10, learning_rate=3, num_steps=100)\n",
    "ax.plot(np.abs(out))\n",
    "ax.set_yscale(\"log\")\n",
    "ax.grid(True)\n",
    "\n",
    "ax.set_ylabel(\"Distance from Minimum\")\n",
    "ax.set_xlabel(\"Number of Steps\")\n",
    "ax.set_title(r\"Analyzing Convergence of Gradient Descent on $\\mathscr{L}(w)=w^2$\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Descent Down a Parabolic Surface in Python**:\n",
    "    \n",
    "Complete the following Python function that implements gradient descent on the skewed paraboloid $\\mathscr{L}(w_1, w_2) = 2 w_1^2 + 3 w_2^2$.\n",
    "\n",
    "Note that the partial derivatives of this function are simply\n",
    "    \n",
    "\\begin{equation}\n",
    "\\frac{\\partial \\mathscr{L}(w_1, w_2)}{\\partial x} = 4 w_1 \\\\\n",
    "\\frac{\\partial \\mathscr{L}(w_1, w_2)}{\\partial y} = 6 w_2 \\\\\n",
    "\\end{equation}\n",
    "\n",
    "Test your function using the inputs `w_start=np.array([2,4])`, `learning_rate=0.1`, and `num_steps=5` and, by hand, confirm that your function correctly performed the first gradient step.\n",
    "Repeat this computation using `xy_start=np.array([-2,-4])`, and see that gradient descent reliably coaxes $\\vec{w}$ in the opposite direction - still towards the global minimum of the paraboloid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def descent_down_2d_parabola(w_start, learning_rate, num_steps):\n",
    "    \"\"\"\n",
    "    Performs gradient descent on L(w1, w2) = 2 * w1 ** 2 + 3 * w2 **2 , \n",
    "    returning the sequence of w-values: [w_start, ..., w_stop]\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    w_start : np.ndarray, shape-(2,)\n",
    "        The initial value of (w1, w2).\n",
    "\n",
    "    learning_rate : float\n",
    "        The \"learning rate\" factor for each descent step. A positive number.\n",
    "\n",
    "    num_steps : int\n",
    "        The number subsequent of descent steps taken. A non-negative number.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray, shape-(num_steps + 1, 2)\n",
    "        The sequence of (w1, w2)-values produced via gradient descent, starting \n",
    "        with w_start\n",
    "    \"\"\"\n",
    "    xy_values = [w_start]\n",
    "    for _ in range(num_steps):\n",
    "        xy_old = xy_values[-1]\n",
    "        xy_new = xy_old - learning_rate * (np.array([4., 6.]) * xy_old)\n",
    "        xy_values.append(xy_new)\n",
    "    return np.array(xy_values)\n",
    "\n",
    ">>> descent_down_2d_parabola((2, 4), 0.1, 5)\n",
    "array([[2.     , 4.     ],\n",
    "       [1.2    , 1.6    ],\n",
    "       [0.72   , 0.64   ],\n",
    "       [0.432  , 0.256  ],\n",
    "       [0.2592 , 0.1024 ],\n",
    "       [0.15552, 0.04096]])\n",
    "\n",
    ">>> descent_down_2d_parabola((-2, -4), 0.1, 5)\n",
    "array([[-2.     , -4.     ],\n",
    "       [-1.2    , -1.6    ],\n",
    "       [-0.72   , -0.64   ],\n",
    "       [-0.432  , -0.256  ],\n",
    "       [-0.2592 , -0.1024 ],\n",
    "       [-0.15552, -0.04096]])\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "markdown",
    "format_version": "1.2",
    "jupytext_version": "1.9.1"
   }
  },
  "kernelspec": {
   "display_name": "Python [conda env:.conda-cogweb]",
   "language": "python",
   "name": "conda-env-.conda-cogweb-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
