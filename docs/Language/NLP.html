

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta content="Topic: Language module, Difficulty: Easy, Category: Section" name="description" />
<meta content="natural language processing, artificial intelligence" name="keywords" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Natural Language Processing &mdash; CogWorks</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/my_theme.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Word Embeddings and Autoencoders" href="WordEmbeddings.html" />
    <link rel="prev" title="Prerequisites" href="prereqs.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> CogWorks
          

          
          </a>

          
            
            
              <div class="version">
                1.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Table of Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../intro.html">CogWorks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pre_reqs.html">Course Pre-Requisites</a></li>
<li class="toctree-l1"><a class="reference internal" href="../supplemental_math.html">Supplemental Math Materials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../audio.html">Audio Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../vision.html">Vision Module</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../language.html">Language Module</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="prereqs.html">Prerequisites</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Natural Language Processing</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Introduction-to-AI">Introduction to AI</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Defining-AI">Defining AI</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Milestones-of-AI">Milestones of AI</a></li>
<li class="toctree-l4"><a class="reference internal" href="#The-Landscape-of-AI">The Landscape of AI</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Are-We-Done?">Are We Done?</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#Introduction-to-NLP">Introduction to NLP</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#What-is-NLP">What is NLP</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Levels-for-Analyzing-Language">Levels for Analyzing Language</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Intro-to-Language-Modeling">Intro to Language Modeling</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="WordEmbeddings.html">Word Embeddings and Autoencoders</a></li>
<li class="toctree-l2"><a class="reference internal" href="RNN.html">RNNs</a></li>
<li class="toctree-l2"><a class="reference internal" href="SemanticImageSearch.html">Language Module Capstone: Semantic Image Search</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../cheat_sheet.html">Cheat Sheets</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">CogWorks</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../language.html">Language Module</a> &raquo;</li>
        
      <li>Natural Language Processing</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/Language/NLP.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="Natural-Language-Processing">
<h1>Natural Language Processing<a class="headerlink" href="#Natural-Language-Processing" title="Permalink to this headline">¶</a></h1>
<div class="section" id="Introduction-to-AI">
<h2>Introduction to AI<a class="headerlink" href="#Introduction-to-AI" title="Permalink to this headline">¶</a></h2>
<p>So far, you’ve learned how to get your computers to understand sound and images. This week we’ll be extending that to language. Then you’ll have almost all the pieces for building an AI agent that can hear, see, and communicate with language.</p>
<div class="section" id="Defining-AI">
<h3>Defining AI<a class="headerlink" href="#Defining-AI" title="Permalink to this headline">¶</a></h3>
<p>There are many ways of defining artificial intelligence.</p>
<p>According to Chollet in <em>Deep Learning with Python</em>, AI “automate[s] intellectual tasks normally performed by humans.” In <em>Artificial Intelligence: A Modern Approach</em> by Russell and Norvig, it is described as having two dimensions: either abstract versus physical, or human versus rational. The latter is expanded on, with relation to acting and thinking:</p>
<p><strong>Thinking humanly</strong> is the “cognitive modeling” approach. It determines how humans think through introspection, psychological experiments, and brain imaging. The idea is that once a theory of the mind is formulated, it can be expressed as a computer program.</p>
<p><strong>Thinking rationally</strong> is the laws of thought approach. It contains precise notation for statements about objects and relations among them. There are rules for yielding correct conclusions from premises i.e., logic.</p>
<p><strong>Acting humanly</strong> is the “Turing Test” approach. A computer passes the Turing Test if a human interrogator can not tell whether written responses come from a person or a computer. The total Turing Test includes video feed and hatch for passing physical objects.</p>
<p><strong>Acting Rationally</strong> is the “rational agent” approach. It creates agents that operate autonomously, perceive the environment, persist, adapt, and create and pursue goals. Rational agents act to achieve the best outcome (or best expected outcome in face of uncertainty).</p>
<p>These four bring up questions. For example, in the history of flight: did we need to model flying machines on nature? Or did we need to just solve the problem of flight in whatever way worked? For self-driving cars: do we want them to drive optimally, or mimic human drivers? This is just something to keep in mind as we’re building intelligent machines.</p>
</div>
<div class="section" id="Milestones-of-AI">
<h3>Milestones of AI<a class="headerlink" href="#Milestones-of-AI" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>1997 IBM Deep Blue defeats reigning chess champion (Garry Kasparov)</p></li>
<li><p>2002 iRobot’s Roomba autonomously vacuums the floor while navigating and avoiding obstacles</p></li>
<li><p>2004 NASA’s robotic exploration rovers Spirit and Opportunity autonomously navigate the surface of Mars</p></li>
<li><p>2007 DARPA Grand Challenge (“Urban Challenge”): for autonomous cars to obey traffic rules and operate in an urban environment</p></li>
<li><p>2009 Robot Scientist “Adam” discovers new scientific knowledge, and it is able to perform independent experiments to test hypotheses and interpret findings</p></li>
<li><p>2011 IBM Watson defeats former Jeopardy! champions (Brad Rutter and Ken Jennings)</p></li>
<li><p>2014 “Eugene” (program that simulates a 13 year old boy) passes Turing Test at University of Reading event (mistaken for human by more than 30% of judges)</p></li>
<li><p>2015 DeepMind achieves human expert level of play on Atari games (using only raw pixels and scores)</p></li>
<li><p>2016 DeepMind AlphaGo defeats top human Go player (Lee Sedol)</p></li>
<li><p>2018 IBM Project Debater</p></li>
</ul>
<p>The following are virtual assistants that use natural language to answer questions, make recommendations and perform actions.</p>
<ul class="simple">
<li><p>2011 Apple’s Siri virtual assistant released (iPhone 4S)</p></li>
<li><p>2014 Amazon Alexa virtual assistant released (Echo. It can control smart devices and be extended with skills)</p></li>
</ul>
</div>
<div class="section" id="The-Landscape-of-AI">
<h3>The Landscape of AI<a class="headerlink" href="#The-Landscape-of-AI" title="Permalink to this headline">¶</a></h3>
<p>As we have seen, there are lots of AI techniques used to achieve amazing feats! The diagram below (Chollet Fig 1.1) captures how AI, machine learning, and deep learning are related.</p>
<div style="text-align: center">
<p>
<img src="../_images/AI.png" alt="ML, AI, Deep Learning relationship" width=500>
</p>
</div><p>The current concepts on AI are from the 1950s. The Turing Test is from 1950 and in 1956, the term <em>artificial intelligence</em> was coined in the Dartmouth College summer AI conference. AI encompasses “symbolic AI”, which is a set of explicit hard-coded rules for manipulating knowledge. This is found in expert systems for medical diagnosis and traditional chess programs. Examples of artificial intelligence include the minimax search and theorem proving.</p>
<p>Machine learning is the ability to learn from experience, and its workings are delineated in the figure below.</p>
<div style="text-align: center">
<p>
<img src="../_images/ML.png" alt="ML diagram" width=500>
</p>
</div><p>Machine learning focuses on training rather than programming. The techniques used include decision trees, logistic regression, and support vector machines. Examples of ML include decision trees and support vectors machines.</p>
<p>Deep learning, which is a form of machine learning, is where we are. It originates from the early 2010s and is hierarchical representation learning. This means that learning occurs in successive layers of increasingly meaningful representations that get closer to expected output. There has been a lot of progress and several breakthroughs in image classification, speech recognition, machine translation, autonomous driving, and game playing.</p>
</div>
<div class="section" id="Are-We-Done?">
<h3>Are We Done?<a class="headerlink" href="#Are-We-Done?" title="Permalink to this headline">¶</a></h3>
<p>Well, we are still far from human-level artificial <em>general</em> intelligence. Tests associated with this are the coffee test formulated by Wozniak and the flat pack furniture test by Tony Severyns. In the coffee test, a machine is required to enter an average American home and figure out how to make coffee: find the coffee machine, find the coffee, add water, find a mug, and brew the coffee by pushing the proper buttons. In the flat pack furniture test, a machine is required to unpack and assemble
an item of flat-packed furniture. It has to read the instructions and assemble the item as described, correctly installing all fixtures.</p>
</div>
</div>
<div class="section" id="Introduction-to-NLP">
<h2>Introduction to NLP<a class="headerlink" href="#Introduction-to-NLP" title="Permalink to this headline">¶</a></h2>
<div class="section" id="What-is-NLP">
<h3>What is NLP<a class="headerlink" href="#What-is-NLP" title="Permalink to this headline">¶</a></h3>
<p>Natural language processing, or NLP, is a subfield of AI that deals with techniques and systems for handling spoken or written language. Natural language understanding, NLU, and natural language generation, NLG, are branches of NLP. It is important to consider what <em>language</em> means. According to Wikipedia, “Language is the ability to acquire and use complex systems of communication, particularly the human ability to do so.” We will not be diving too deep into linguistics. We will focus on how to
get computers to do things that we would normally consider requiring the ability to understand and use language and even the ability to think.</p>
<p>Some of the tasks NLP can be used for are:</p>
<ul class="simple">
<li><p>answering questions</p></li>
<li><p>natural language query interfaces</p></li>
<li><p>image captioning (spanning both computer vision and language)</p></li>
<li><p>authorship ID (identifying the author(s) of different works)</p></li>
<li><p>plagiarism detection</p></li>
<li><p>automatic essay grading</p></li>
<li><p>dialog systems (a computer conversing with a human)</p></li>
<li><p>voice user interfaces (speech-to-text and voice commands for technology)</p></li>
<li><p>document summarization</p></li>
<li><p>document retrieval (based on keyword searches, etc)</p></li>
<li><p>document classification (based on main topic(s) covered)</p></li>
<li><p>document clustering (grouping similar documents)</p></li>
<li><p>document recommendation (based on history of documents read, etc)</p></li>
</ul>
</div>
<div class="section" id="Levels-for-Analyzing-Language">
<h3>Levels for Analyzing Language<a class="headerlink" href="#Levels-for-Analyzing-Language" title="Permalink to this headline">¶</a></h3>
<p>At the most basic level, language is a signal or sequence that is either analog (speech) or digital (text). We will be focusing mostly on text, of which there is an abundance due to the internet.</p>
<p>Here is a sample sentence: “A dog is chasing a boy on the playground.” We can view and analyze it on several levels of increasing complexity: sequence of letters from an alphabet, sequence of words (note that not all languages have easily recognizable word boundaries, e.g., Chinese), part-of-speech tags, syntactic structure, entities and relationships, and logic predicates.</p>
<div style="text-align: center">
<p>
<img src="../_images/text_analysis.png" alt="different ways of breaking down text" width=500>
</p>
</div><p>The farther down we go, the closer we get to extracting and representing the knowledge inherent in the sentence. We will be starting at a higher level because there is a lot that can be done while not explicitly dealing with parsing the tree of a sentence. Similar to using raw pixels for computer vision, deep learning methods often can use raw words or characters as inputs. This is possible because neural networks can learn hierarchical features automatically.</p>
<p>However, there are intricacies that make processing language hard. Ambiguity is one issue. For example the sentence “We saw her duck.” could be referring to an aquatic bird, someone avoiding being hit, or a cutting tool. Sarcasm is another issue. For example, double negatives like “I don’t not like it” convey a different meaning from their non double negative counterparts like “I do like it.”</p>
</div>
<div class="section" id="Intro-to-Language-Modeling">
<h3>Intro to Language Modeling<a class="headerlink" href="#Intro-to-Language-Modeling" title="Permalink to this headline">¶</a></h3>
<p>When modeling language, we can think about the probability of a sequence of words or characters:</p>
<div class="math notranslate nohighlight">
\begin{equation}
P(w_{1}, ..., w_{T})= P(w_{1})*P(w_{2}|w_{1})*P(w_{3}|w_{2}, w_{1})*...*P(w_{T}|w_{1}...w_{T-1})
\end{equation}</div><p>For example, in English, “the black cat” is much more likely to occur than “the cat black.” This model could be used in language id: how likely is a piece of text to be from a particular language? It could be used in machine translation: after generating a bunch of possible translations for a sentence, they could be ranked according to how natural (or probable) they are according to a language model of the target language.</p>
<p>An important type of language model is called the n-gram language model. This truncated “history” to (n-1) words.</p>
<div class="math notranslate nohighlight">
\begin{equation}
P(w_i|w_1, ..., w_{i-1}) = P(w_i|w_{i-(n-1)}, ..., w_{i-1})
\end{equation}</div><p>So for a trigram model: <span class="math notranslate nohighlight">\(P(w_i|w_{i-2}, w_{i-1})\)</span>, which in our case would be: <span class="math notranslate nohighlight">\(P(cat|the, black)\)</span></p>
<p>These probabilities can be estimated by counting relative frequencies in a training corpus.</p>
<p>A different approach is the “bag of words” approach. In this model, the frequencies within a set of words are measured across different pieces of text. It is important to note that order and grammar are lost. In addition, the set of words the model tracks must be chosen wisely. For example, removing generally meaningless words that provide grammatical structure, like “the” and “on”, is good practice. These words are referred to as <em>stop words</em>. This model can be used in machine learning for
document classification. Inferences can be drawn on the contents of a document based on combinations of different frequencies. For example, a document with the most frequent word being “slide”, the second most frequent word being “swing”, and the third being “kids” might be describing a playground.</p>
<div style="text-align: center">
<p>
<img src="../_images/bag_of_words.png" alt="bag of words representation" width=500>
</p>
</div><p>Some of the conclusions that could be drawn from the bag of words representations above is that documents one and two are more similar due to a bigger overlap in their key terms. They could be classified as describing common pets while document three could be classified as describing taekwondo.</p>
<p>When thinking about choosing a good vocabulary, <strong>Zipf’s Law</strong> helps to quantitatively show which set of words to concentrate on. For the data that Zipf’s Law applies to, if the elements in a <em>large</em> data set are ranked with the element in rank one being the most common, there is an inverse proportional relationship between the frequency and rank of each element. Zipf’s Law applies to both word and character frequencies. In other words, the most frequent word in a large corpus occurs twice as
much as the second most common word, three times as much as the third most common word, and so on. This means that half of the corpus is comprised of one word.</p>
<div style="text-align: center">
<p>
<img src="../_images/zipfs_law.png" alt="Zipf's Law" width=500>
</p>
</div><p>Therefore, it is best to ignore the most and least common words. The most common words tend to be “glue” words like <em>and</em>, <em>a</em>, and <em>the</em> and provide no insight into how to distinguish between sets of texts. The least common words might be too specialized and end up acting like a fold in a flashcard for a NN. The NN could end up <em>correlating the fold with the answer</em>. This could mean latching onto a word that occurs once or twice in the training corpus to correctly classify a specific text which
means it is not learning correctly.</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="WordEmbeddings.html" class="btn btn-neutral float-right" title="Word Embeddings and Autoencoders" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="prereqs.html" class="btn btn-neutral float-left" title="Prerequisites" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Ryan Soklaski

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>