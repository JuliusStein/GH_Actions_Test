

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta content="Topic: Machine learning, Category: Discussion" name="description" />
<meta content="supervised learning, machine learning modeling" name="keywords" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Supervised Learning Using Gradient Descent &mdash; CogWorks</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/my_theme.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Vision Module Capstone" href="FacialRecognition.html" />
    <link rel="prev" title="Where is the “Learning” in All of This?" href="What_Does_Learning_Mean.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> CogWorks
          

          
          </a>

          
            
            
              <div class="version">
                1.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Table of Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../intro.html">CogWorks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pre_reqs.html">Course Pre-Requisites</a></li>
<li class="toctree-l1"><a class="reference internal" href="../supplemental_math.html">Supplemental Math Materials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../audio.html">Audio Module</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../vision.html">Vision Module</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="prereqs.html">Prerequisites</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro_ml.html">A Brief Introduction to Machine Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="Linear_Regression.html">Baby Steps Towards Machine Learning: Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="Exercises/Data_Exploration.html">Exercises: Exploring A Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="Gradient_Descent.html">Gradient-Based Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="Automatic_Differentiation.html">Automatic Differentiation</a></li>
<li class="toctree-l2"><a class="reference internal" href="Exercises/Linear_Regression_Exercise.html">Exercises: Fitting a Linear Model with Gradient Descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="What_Does_Learning_Mean.html">Where is the “Learning” in All of This?</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Supervised Learning Using Gradient Descent</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Dissecting-the-Framework">Dissecting the Framework</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#The-Data">The Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="#The-Model">The Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="#The-Supervisor">The Supervisor</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#Reading-Comprehension-Exercise-Solutions">Reading Comprehension Exercise Solutions</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="FacialRecognition.html">Vision Module Capstone</a></li>
<li class="toctree-l2"><a class="reference internal" href="Whispers.html">Whispers Algorithm</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../language.html">Language Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cheat_sheet.html">Cheat Sheets</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">CogWorks</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../vision.html">Vision Module</a> &raquo;</li>
        
      <li>Supervised Learning Using Gradient Descent</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/Video/Supervised_Learning_and_Modeling.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="Supervised-Learning-Using-Gradient-Descent">
<h1>Supervised Learning Using Gradient Descent<a class="headerlink" href="#Supervised-Learning-Using-Gradient-Descent" title="Permalink to this headline">¶</a></h1>
<p>We used gradient descent to fit a linear model against recorded height-versus-wingspan data for NBA players. Even though <a class="reference external" href="https://rsokl.github.io/CogWeb/Video/What_Does_Learning_Mean.html">we only really performed linear regression here</a>, the framework for how we solved this problem is quite general and is of much practical important to the field of machine learning. It is referred to as the framework of <strong>supervised learning</strong>, and it is presently one of the most popular approaches to
solving “real world” machine learning problems.</p>
<p>Let’s take some time to study the framework for supervised learning. This will lead define some key concepts that crop up all of the time from the lexicon of machine learning; i.e.</p>
<ul class="simple">
<li><p>What <strong>supervision</strong> means in the context of “supervised learning”.</p></li>
<li><p>What is meant by the oft-used term <strong>training</strong>, and how to understand the difference between the phase of training a model versus evaluating a model or using it “in deployment”.</p></li>
</ul>
<p>Our overview of this framework will also lead us to identify the all-important <strong>modeling problem</strong> in machine learning, <strong>which is the motivating problem for the invention of modern neural networks</strong>. By the end of this discussion we will have officially crossed the bridge over into the land of <strong>deep learning</strong>, where we will leverage “deep” neural networks to help us solve machine learning problems.</p>
<div class="section" id="Dissecting-the-Framework">
<h2>Dissecting the Framework<a class="headerlink" href="#Dissecting-the-Framework" title="Permalink to this headline">¶</a></h2>
<div style="text-align: center">
<p>
<img src="../_images/supervised_learning.png" alt="A diagram detailing the supervised learning framework" width="800">
</p>
</div><p>We must keep in mind our overarching objective here: given some piece of observed data, we want to arrive at some mathematical model (typically encapsulated by a computer program) that can produce a useful prediction or decision based on that piece of observed data. The process of learning involves tuning the numerical parameters of our model so that it produces reliable predictions or decisions when it encounters new pieces of observed data. This tuning process is frequently described as
<strong>training</strong> one’s model.</p>
<div class="section" id="The-Data">
<h3>The Data<a class="headerlink" href="#The-Data" title="Permalink to this headline">¶</a></h3>
<p>In the context of supervised learning, <strong>we will need access to a dataset consisting of representative pieces of observed data along with the desired predictions or decisions that we would like our model to make</strong> when it encounters these pieces of data. Such a collection of observed data and associated desired outputs (or “truth data”, to be succinct) is used to form <strong>training, validation, and testing datasets</strong>, which, respectively, are used to train the model directly, to help refine the
hyperparameters used to train the model, and to give us a quantitative measure of how well we expect our model to perform when it encounters brand-new pieces of observed data.</p>
<p>In our worked example, we had access to measured heights and wingspans of rookie NBA players and this served as our training data (we did not go through the process of validation or testing for this preliminary example). The heights served as pieces of observed data, and the recorded wingspans is the associated “truth data”, which, roughly speaking, are the values that we want our model to produce in correspondence with the heights. If we were interested in developing a mathematical model that
can classify images (e.g. an example of a two-class image classification problem is: given the pixels of this image decide whether the picture contains a cat or a dog), then our data set would consist of images that we have collected along with associated labels for the images; the labels are the “truth data” which detail what “class” each image belongs to.</p>
<p>The fact that we have access to “truth data” is what qualifies this as a <strong>supervised</strong> learning framework. If we do not have access to truth data for our problem, then we cannot tackle the problem using supervised learning (instead we could leverage unsupervised and self-supervised learning methods). But in what way is our model learning under our supervision? This is where the <strong>loss function and gradient descent</strong> come into play, which we will get to shortly.</p>
</div>
<div class="section" id="The-Model">
<h3>The Model<a class="headerlink" href="#The-Model" title="Permalink to this headline">¶</a></h3>
<p>Our model is the thing that mediates the transformation of a piece of observed data to a prediction or decision; in this way, it is the “intelligent” part of this framework. While in practice the model inevitably takes form as an algorithm implemented by a computer program, it is most useful to just think of it as a mathematical function</p>
<div class="math notranslate nohighlight">
\begin{equation}
F\big((w_1, \dots, w_{M}); x\big) = y^{(\mathrm{pred})}
\end{equation}</div><p>where <span class="math notranslate nohighlight">\(F\)</span> is the function that transforms an observation (<span class="math notranslate nohighlight">\(x\)</span>) to an output (<span class="math notranslate nohighlight">\(y^{(\mathrm{pred})}\)</span>), and <span class="math notranslate nohighlight">\((w_{j})_{j=1}^M\)</span> is the collection of tunable parameters associated with this function. Recall that our goal is to find a numerical value for each of these <span class="math notranslate nohighlight">\(M\)</span> parameters so that our model will make reliable predictions or decisions when it encounters new data. Let’s represent a collection of such “optimal” parameter values as <span class="math notranslate nohighlight">\((w^{*}_{j})_{j=1}^M\)</span>, then
<span class="math notranslate nohighlight">\(F\big((w^{*}_1, \dots, w^{*}_{M}); x\big)\)</span> represents our <strong>trained model</strong>.</p>
<p>In the context of predicting a NBA player’s wingspan based only on his height, we used the simple linear model:</p>
<div class="math notranslate nohighlight">
\begin{equation}
F\big((w_1, w_2); x\big) = w_2 x + w_1
\end{equation}</div><p>And once we found satisfactory values for the slope (<span class="math notranslate nohighlight">\(w^{*}_2\)</span>) and y-intercept (<span class="math notranslate nohighlight">\(w^{*}_1\)</span>) that produced a line closely fit our training data, we had arrived at our “trained” linear model.</p>
<p>But how do we write down a sensible form for <span class="math notranslate nohighlight">\(F\big((w_1, \dots, w_{M}); x\big)\)</span> when we can’t simply plot our data and plainly identify patterns shared between the inputs and outputs? In the aforementioned image classification problem, <span class="math notranslate nohighlight">\(x\)</span> is the pixels of an image, and <span class="math notranslate nohighlight">\(F\)</span> needs to process those pixels and return a numerical score that represents some measure of “dog-ness” or “cat-ness” for that image… I don’t know about you, but it is not obvious to me how I would write down
<span class="math notranslate nohighlight">\(F\)</span> for this problem! This is what we will refer to as <strong>the modeling problem</strong>.</p>
<p>The <strong>booming success of neural networks is that it helps us solve the modelling problem</strong>: people have discovered simple, composable mathematical building blocks (often referred to as “layers of neurons”) that can be stacked together to create a highly “sculptable” model <span class="math notranslate nohighlight">\(F\)</span> (referred to as a “deep neural network”). Whereas in the previous example we were responsible for specifically choosing to use a linear model for our problem, we can instead use a neural network model, whose form is
shaped chiefly by the data that we use to train it. Prior to being trained, the neural network is formless like a block of clay, and the training process can be thought as the data sculpting the model so that it captures the important patterns and relationships shared by our observed data and the desired predictions/decisions that we want our model to make. In this way, the trained neural network can reliably map new observations to useful predictions and decisions, based on the patterns that
were “sculpted” into it.</p>
</div>
<div class="section" id="The-Supervisor">
<h3>The Supervisor<a class="headerlink" href="#The-Supervisor" title="Permalink to this headline">¶</a></h3>
<p>The supervisor is responsible for comparing our model’s prediction against the “true” prediction and providing a correction to the model’s parameters in order to incrementally improve the quality of its prediction. In this course, we will inevitably create a <strong>loss function</strong>, <span class="math notranslate nohighlight">\(\mathscr{L}(y^{(\mathrm{pred})}, y^{\mathrm{(true)}})\)</span>, that is responsible for measuring the quality of our model’s predictions. We design this to be a continuous function that compares a prediction against the
“true” result and returns a value that gets smaller as the agreement between the prediction and the “truth” improves. Thus, as we saw before, we want to find the model parameters such that the average loss taken over our dataset is <strong>minimized</strong>:</p>
<div class="math notranslate nohighlight">
\begin{equation}
\frac{1}{N}\sum_{n=0}^{N-1}{\mathscr{L}\big(F\big((w_1, \dots, w_{M}); x_{n}\big), y_{n}^{\mathrm{(true)}}\big)}
\end{equation}</div><p>We search for these optimal model parameters <span class="math notranslate nohighlight">\((w^{*}_1, \dots, w^{*}_{M})\)</span> using gradient descent, where we leverage automatic differentiation through our model and the loss function in order to “measure” each <span class="math notranslate nohighlight">\(\frac{\mathrm{d}\mathscr{L}}{\mathrm{d} w_i}\)</span>.</p>
<div class="admonition warning">
<p class="admonition-title fa fa-exclamation-circle"><strong>Training on Batches of Data</strong>:</p>
<p>The diagram above shows us feeding the model a single piece of training data, and updating the model based on the output associated with that datum. In practice, we will often feed the model a “batch” of data – consisting of <span class="math notranslate nohighlight">\(n\)</span> pieces of input data, where <span class="math notranslate nohighlight">\(n\)</span> is the “batch size” – and it will process each piece of data in the batch independently, producing <span class="math notranslate nohighlight">\(n\)</span> corresponding outputs. It is also common to assemble this batch by drawing the examples at random from our pool of
training data. Our loss function will then measure the quality of the model’s <span class="math notranslate nohighlight">\(n\)</span> predictions <em>averaged over the batch of predictions</em>. Thus the gradient-based updates made to our model’s weights will be informed not by a single prediction but by an ensemble of predictions.</p>
<p>This has multiple benefits. First and foremost, by using a batch of data that has been randomly sampled from our dataset, we will find ourselves with gradients that more consistently (and “smoothly”) move our model’s weights towards an optimum configuration. The gradients associated with two different pieces of training data might vary significantly from each other, and thus could lead to a “noisy” or highly tumultuous sequence of updates to our model’s weights were we to use a batch of size
<span class="math notranslate nohighlight">\(1\)</span>. This issue is mitigated if the gradient is instead derived from a loss averaged over multiple pieces of data, where the “noisy” components of the gradient are able to cancel each other out in the aggregate and thus the gradient can more reliably steer us down the loss landscape.</p>
<p>Second, there are often times computational benefits to processing batches of data. For languages like Python, it is critical to be able to leverage <a class="reference external" href="https://www.pythonlikeyoumeanit.com/Module3_IntroducingNumpy/VectorizedOperations.html">vectorization</a> through libraries like PyTorch and NumPy, in order to efficiently perform numerical processing. Batched processing naturally enables vectorized processing.</p>
<p>A final, but important note on terminology: the phrase <strong>“stochastic gradient descent”</strong> is often used to refer to this style of batched processing to drive gradient-based supervised learning. “Stochastic” is a fancy way of saying “random”, and it is alluding to the process of building up a batch of data by randomly sampling from one’s training data.</p>
</div>
<div class="admonition note">
<p class="admonition-title fa fa-exclamation-circle"><strong>Reading Comprehension: Filling Out the Supervised Learning Diagram</strong></p>
<p>Reflect, once again, on the height-versus-wingspan modeling problem that we tackled. Step through the supervised learning diagram above, and fill out the various abstract labels with the particulars of that toy problem.</p>
<p>What is..</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(x_i\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(y^{\mathrm{(true)}}_i\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(F\big((w_1, \dots, w_{M}); x\big)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(y^{\mathrm{(pred)}}_i\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathscr{L}(y^{(\mathrm{pred})}, y^{\mathrm{(true)}})\)</span></p></li>
<li><p>And how did we access each <span class="math notranslate nohighlight">\(\frac{\mathrm{d}\mathscr{L}}{\mathrm{d} w_i}\)</span> to form the gradient, to update our model? Did we write these derivatives out by hand?</p></li>
</ul>
</div>
</div>
</div>
<div class="section" id="Reading-Comprehension-Exercise-Solutions">
<h2>Reading Comprehension Exercise Solutions<a class="headerlink" href="#Reading-Comprehension-Exercise-Solutions" title="Permalink to this headline">¶</a></h2>
<p><strong>Filling Out the Supervised Learning Diagram: Solution</strong></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(x_i\)</span>: is a height from our training data. I.e. it is the height of one of the players from our dataset.</p></li>
<li><p><span class="math notranslate nohighlight">\(y^{\mathrm{(true)}}_i\)</span>: is the corresponding wingspan that we measured for that same player; it is what we would <em>like</em> our model to predict.</p></li>
<li><p><span class="math notranslate nohighlight">\(F\big((w_1, \dots, w_{M}); x\big)\)</span>: is out linear model <span class="math notranslate nohighlight">\(w_2 x + w_1\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(y^{\mathrm{(pred)}}_i\)</span>: is <span class="math notranslate nohighlight">\(w_2 x_i + w_1\)</span>, which is the predicted wingspan that our model produced based on the current values of its parameters <span class="math notranslate nohighlight">\(w_1\)</span> and <span class="math notranslate nohighlight">\(w_2\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathscr{L}(y^{(\mathrm{pred})}, y^{\mathrm{(true)}})\)</span> is the mean-squared error, which we use to measure the discrepancy between our predicted wingspan and the true wingspan</p></li>
</ul>
<div class="math notranslate nohighlight">
\begin{equation}
\mathscr{L}_{\mathrm{MSE}} = \frac{1}{N}\sum_{n=0}^{N-1}{\big(y^{\mathrm{(true)}}_n - y^{\mathrm{(pred)}}_n\big)^2}
\end{equation}</div><ul class="simple">
<li><p>We gained access to each <span class="math notranslate nohighlight">\(\frac{\mathrm{d}\mathscr{L}}{\mathrm{d} w_i}\)</span> (in order to perform gradient descent) by leveraging the automatic differentiation library MyGrad.</p></li>
</ul>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="FacialRecognition.html" class="btn btn-neutral float-right" title="Vision Module Capstone" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="What_Does_Learning_Mean.html" class="btn btn-neutral float-left" title="Where is the “Learning” in All of This?" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Ryan Soklaski

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>